{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:44:12.102399Z",
     "start_time": "2021-04-16T15:44:10.352926Z"
    }
   },
   "outputs": [],
   "source": [
    "#将kilometer当做类别变量处理试试,异常值用groupby处理,'匿名特征可以进一步处理一下'\n",
    "## 基础工具\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import jn\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "## 模型预测的\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "\n",
    "## 数据降维处理的\n",
    "from sklearn.decomposition import PCA,FastICA,FactorAnalysis,SparsePCA\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "## 参数搜索和评价的\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import scipy.signal as signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:44:31.485584Z",
     "start_time": "2021-04-16T15:44:31.460679Z"
    }
   },
   "outputs": [],
   "source": [
    "#处理异常值\n",
    "def smooth_cols(group,out_value,kind):\n",
    "    cols = ['power']\n",
    "    if kind == 'g':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]<out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.995))\n",
    "        return group\n",
    "    if kind == 'l':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]>out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.07))\n",
    "        return group        \n",
    "def date_proc(x):\n",
    "    m = int(x[4:6])\n",
    "    if m == 0:\n",
    "        m = 1\n",
    "    return x[:4] + '-' + str(m) + '-' + x[6:]\n",
    "\n",
    "#定义日期提取函数\n",
    "def date_tran(df,fea_col):\n",
    "    for f in tqdm(fea_col):\n",
    "        df[f] = pd.to_datetime(df[f].astype('str').apply(date_proc))\n",
    "        df[f + '_year'] = df[f].dt.year\n",
    "        df[f + '_month'] = df[f].dt.month\n",
    "        df[f + '_day'] = df[f].dt.day\n",
    "        df[f + '_dayofweek'] = df[f].dt.dayofweek\n",
    "    return (df)\n",
    "\n",
    "#分桶操作\n",
    "def cut_group(df,cols,num_bins=50):\n",
    "    for col in cols:\n",
    "        all_range = int(df[col].max()-df[col].min())\n",
    "        bin = [i*all_range/num_bins for i in range(all_range)]\n",
    "        df[col+'_bin'] = pd.cut(df[col], bin, labels=False)\n",
    "    return df\n",
    "\n",
    "### count编码\n",
    "def count_coding(df,fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return(df)\n",
    "#定义交叉特征统计\n",
    "def cross_cat_num(df,num_col,cat_col):\n",
    "    for f1 in tqdm(cat_col):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_col):\n",
    "            feat = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max', '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "            })\n",
    "            df = df.merge(feat, on=f1, how='left')\n",
    "    return(df)\n",
    "### 类别特征的二阶交叉\n",
    "from scipy.stats import entropy\n",
    "def cross_qua_cat_num(df):\n",
    "    for f_pair in tqdm([\n",
    "        ['model', 'brand'], ['model', 'regionCode'], ['brand', 'regionCode']\n",
    "    ]):\n",
    "        ### 共现次数\n",
    "        df['_'.join(f_pair) + '_count'] = df.groupby(f_pair)['SaleID'].transform('count')\n",
    "        ### n unique、熵\n",
    "        df = df.merge(df.groupby(f_pair[0], as_index=False)[f_pair[1]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[0], f_pair[1]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[0], f_pair[1]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[0], how='left')\n",
    "        df = df.merge(df.groupby(f_pair[1], as_index=False)[f_pair[0]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[1], f_pair[0]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[1], f_pair[0]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[1], how='left')\n",
    "        ### 比例偏好\n",
    "        df['{}_in_{}_prop'.format(f_pair[0], f_pair[1])] = df['_'.join(f_pair) + '_count'] / df[f_pair[1] + '_count']\n",
    "        df['{}_in_{}_prop'.format(f_pair[1], f_pair[0])] = df['_'.join(f_pair) + '_count'] / df[f_pair[0] + '_count']\n",
    "    return (df)\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:45:20.212504Z",
     "start_time": "2021-04-16T15:45:18.552185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 37200080.00 MB\n",
      "Memory usage after optimization is: 10200184.00 MB\n",
      "Decreased by 72.6%\n",
      "Memory usage of dataframe is 12000080.00 MB\n",
      "Memory usage after optimization is: 3200184.00 MB\n",
      "Decreased by 73.3%\n",
      "Train data shape: (150000, 31)\n",
      "TestA data shape: (50000, 30)\n",
      "concat_data shape: (200000, 31)\n"
     ]
    }
   ],
   "source": [
    "## 通过Pandas对于数据进行读取 (pandas是一个很友好的数据读取函数库)\n",
    "Train_data = reduce_mem_usage(pd.read_csv('./used_car_train_20200313.csv', sep=' '))\n",
    "TestA_data = reduce_mem_usage(pd.read_csv('./used_car_testB_20200421.csv', sep=' '))\n",
    "\n",
    "#Train_data = Train_data[Train_data['price']>100]\n",
    "#Train_data['price'] = np.log1p(Train_data['price'])\n",
    "## 输出数据的大小信息\n",
    "print('Train data shape:',Train_data.shape)\n",
    "print('TestA data shape:',TestA_data.shape)\n",
    "\n",
    "\n",
    "#合并数据集\n",
    "concat_data = pd.concat([Train_data,TestA_data])\n",
    "concat_data['notRepairedDamage'] = concat_data['notRepairedDamage'].replace('-',0).astype('float16')\n",
    "concat_data = concat_data.fillna(concat_data.mode().iloc[0,:])\n",
    "#concat_data.index = range(200000)\n",
    "#concat_data = concat_data.groupby('bodyType').apply(smooth_cols,out_value=600,kind='g')\n",
    "#concat_data.index = range(200000)\n",
    "#concat_data['power'] = np.log(concat_data['power'])\n",
    "print('concat_data shape:',concat_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:45:29.544334Z",
     "start_time": "2021-04-16T15:45:29.400606Z"
    }
   },
   "outputs": [],
   "source": [
    "#截断异常值\n",
    "concat_data['power'][concat_data['power']>600] = 600\n",
    "concat_data['power'][concat_data['power']<1] = 1\n",
    "\n",
    "concat_data['v_13'][concat_data['v_13']>6] = 6\n",
    "concat_data['v_14'][concat_data['v_14']>4] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:45:36.998954Z",
     "start_time": "2021-04-16T15:45:36.264401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 353)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in ['v_' +str(i) for i in range(14)]:\n",
    "    for j in ['v_' +str(i) for i in range(14)]:\n",
    "        concat_data[str(i)+'+'+str(j)] = concat_data[str(i)]+concat_data[str(j)]\n",
    "for i in ['model','brand', 'bodyType', 'fuelType','gearbox', 'power', 'kilometer', 'notRepairedDamage', 'regionCode']:\n",
    "    for j in ['v_' +str(i) for i in range(14)]:\n",
    "        concat_data[str(i)+'*'+str(j)] = concat_data[i]*concat_data[j]    \n",
    "concat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:45:51.875766Z",
     "start_time": "2021-04-16T15:45:51.171164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#提取日期信息\n",
    "date_cols = ['regDate', 'creatDate']\n",
    "concat_data = date_tran(concat_data,date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:46:14.698991Z",
     "start_time": "2021-04-16T15:46:14.387320Z"
    }
   },
   "outputs": [],
   "source": [
    "data = concat_data.copy()\n",
    "\n",
    "#count编码\n",
    "count_list = ['regDate', 'creatDate', 'model', 'brand', 'regionCode','bodyType','fuelType','name','regDate_year', 'regDate_month', 'regDate_day',\n",
    "       'regDate_dayofweek' , 'creatDate_month','creatDate_day', 'creatDate_dayofweek','kilometer']\n",
    "       \n",
    "data = count_coding(data,count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:46:23.340966Z",
     "start_time": "2021-04-16T15:46:23.255693Z"
    }
   },
   "outputs": [],
   "source": [
    "#特征构造\n",
    "# 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比\n",
    "# 不过要注意，数据里有时间出错的格式，所以我们需要 errors='coerce'\n",
    "data['used_time1'] = (pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                            pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "data['used_time2'] = (pd.datetime.now() - pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days                        \n",
    "data['used_time3'] = (pd.datetime.now() - pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') ).dt.days\n",
    "\n",
    "#分桶操作\n",
    "cut_cols = ['power']+['used_time1','used_time2','used_time3']\n",
    "data = cut_group(data,cut_cols,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:46:44.218311Z",
     "start_time": "2021-04-16T15:46:31.725138Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      " 17%|█▋        | 1/6 [00:01<00:08,  1.63s/it]\n",
      " 33%|███▎      | 2/6 [00:02<00:05,  1.26s/it]\n",
      " 50%|█████     | 3/6 [00:02<00:02,  1.00it/s]\n",
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.22it/s]\n",
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.44it/s]\n",
      " 33%|███▎      | 1/3 [00:03<00:07,  3.64s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      " 17%|█▋        | 1/6 [00:02<00:10,  2.03s/it]\n",
      " 33%|███▎      | 2/6 [00:02<00:06,  1.54s/it]\n",
      " 50%|█████     | 3/6 [00:02<00:03,  1.19s/it]\n",
      " 67%|██████▋   | 4/6 [00:03<00:01,  1.05it/s]\n",
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.27it/s]\n",
      " 67%|██████▋   | 2/3 [00:07<00:03,  3.79s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      " 17%|█▋        | 1/6 [00:02<00:12,  2.46s/it]\n",
      " 33%|███▎      | 2/6 [00:02<00:07,  1.84s/it]\n",
      " 50%|█████     | 3/6 [00:03<00:04,  1.42s/it]\n",
      " 67%|██████▋   | 4/6 [00:03<00:02,  1.11s/it]\n",
      " 83%|████████▎ | 5/6 [00:04<00:00,  1.11it/s]\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.06s/it]\n"
     ]
    }
   ],
   "source": [
    "### 用数值特征对类别特征做统计刻画，随便挑了几个跟price相关性最高的匿名特征\n",
    "cross_cat = ['model', 'brand','regDate_year']\n",
    "cross_num = ['v_0','v_3', 'v_4', 'v_8', 'v_12','power']\n",
    "data = cross_cat_num(data,cross_num,cross_cat)#一阶交叉\n",
    "#data = cross_qua_cat_num(data)#二阶交叉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:47:09.974094Z",
     "start_time": "2021-04-16T15:47:09.689793Z"
    }
   },
   "outputs": [],
   "source": [
    "## 选择特征列\n",
    "numerical_cols = data.columns\n",
    "#print(numerical_cols)\n",
    "\n",
    "cat_fea = ['SaleID','offerType','seller']\n",
    "feature_cols = [col for col in numerical_cols if col not in cat_fea]\n",
    "feature_cols = [col for col in feature_cols if col not in ['price']]\n",
    "\n",
    "## 提前特征列，标签列构造训练样本和测试样本\n",
    "X_data = data.iloc[:len(Train_data),:][feature_cols]\n",
    "Y_data = Train_data['price']\n",
    "X_test  = data.iloc[len(Train_data):,:][feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:47:21.538399Z",
     "start_time": "2021-04-16T15:47:21.518941Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from itertools import product\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=10, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    " \n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    " \n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    " \n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    " \n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    " \n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    " \n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    " \n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    " \n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    " \n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    " \n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    " \n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    " \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    " \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    " \n",
    "        return X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:47:50.886651Z",
     "start_time": "2021-04-16T15:47:31.096478Z"
    }
   },
   "outputs": [],
   "source": [
    "class_list = ['model','brand','name','regionCode']+date_cols\n",
    "MeanEnocodeFeature = class_list#声明需要平均数编码的特征\n",
    "ME = MeanEncoder(MeanEnocodeFeature,target_type='regression') #声明平均数编码的类\n",
    "X_data = ME.fit_transform(X_data,Y_data)#对训练数据集的X和y进行拟合\n",
    "#x_train_fav = ME.fit_transform(x_train,y_train_fav)#对训练数据集的X和y进行拟合\n",
    "X_test = ME.transform(X_test)#对测试集进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:47:50.892670Z",
     "start_time": "2021-04-16T15:47:50.887638Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data['price'] = Train_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:48:21.841941Z",
     "start_time": "2021-04-16T15:47:56.681070Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:25<00:00,  4.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "### target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "enc_cols = []\n",
    "stats_default_dict = {\n",
    "    'max': X_data['price'].max(),\n",
    "    'min': X_data['price'].min(),\n",
    "    'median': X_data['price'].median(),\n",
    "    'mean': X_data['price'].mean(),\n",
    "    'sum': X_data['price'].sum(),\n",
    "    'std': X_data['price'].std(),\n",
    "    'skew': X_data['price'].skew(),\n",
    "    'kurt': X_data['price'].kurt(),\n",
    "    'mad': X_data['price'].mad()\n",
    "}\n",
    "### 暂且选择这三种编码\n",
    "enc_stats = ['max','min','mean']\n",
    "skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for f in tqdm(['regionCode','brand','regDate_year','creatDate_year','kilometer','model']):\n",
    "    enc_dict = {}\n",
    "    for stat in enc_stats:\n",
    "        enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "        X_data['{}_target_{}'.format(f, stat)] = 0\n",
    "        X_test['{}_target_{}'.format(f, stat)] = 0\n",
    "        enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(X_data, Y_data)):\n",
    "        trn_x, val_x = X_data.iloc[trn_idx].reset_index(drop=True), X_data.iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['price'].agg(enc_dict)\n",
    "        val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "        test_x = X_test[[f]].merge(enc_df, on=f, how='left')\n",
    "        for stat in enc_stats:\n",
    "            val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            X_data.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values \n",
    "            X_test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:49:14.436370Z",
     "start_time": "2021-04-16T15:49:14.276255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 454)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "drop_list = ['regDate', 'creatDate','brand_power_min', 'regDate_year_power_min']\n",
    "x_train = X_data.drop(drop_list+['price'],axis=1)\n",
    "x_test = X_test.drop(drop_list,axis=1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:49:17.484472Z",
     "start_time": "2021-04-16T15:49:17.018948Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:49:19.887618Z",
     "start_time": "2021-04-16T15:49:19.086354Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#特征归一化\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(pd.concat([x_train,x_test]).values)\n",
    "all_data = min_max_scaler.transform(pd.concat([x_train,x_test]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:49:27.558592Z",
     "start_time": "2021-04-16T15:49:20.897412Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=146)\n",
    "all_pca = pca.fit_transform(all_data)\n",
    "X_pca = all_pca[:len(x_train)]\n",
    "test = all_pca[len(x_train):]\n",
    "y = Train_data['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:49:33.732478Z",
     "start_time": "2021-04-16T15:49:30.608039Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Activation, MaxPool1D, Flatten, Dense\n",
    "from keras.layers import Input, Dense, Concatenate, Reshape, Dropout, merge, Add\n",
    "def NN_model(input_dim):\n",
    "    init = keras.initializers.glorot_uniform(seed=1)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(units=300, input_dim=input_dim, kernel_initializer=init, activation='softplus'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=300, kernel_initializer=init, activation='softplus'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=64, kernel_initializer=init, activation='softplus'))\n",
    "    model.add(Dense(units=32, kernel_initializer=init, activation='softplus'))\n",
    "    model.add(Dense(units=8, kernel_initializer=init, activation='softplus'))\n",
    "    model.add(Dense(units=1))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:49:35.925697Z",
     "start_time": "2021-04-16T15:49:35.916720Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping\n",
    "class Metric(Callback):\n",
    "    def __init__(self, model, callbacks, data):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.callbacks = callbacks\n",
    "        self.data = data\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_begin(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_end(logs)\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        X_train, y_train = self.data[0][0], self.data[0][1]\n",
    "        y_pred3 = self.model.predict(X_train)\n",
    "        y_pred = np.zeros((len(y_pred3), ))\n",
    "        y_true = np.zeros((len(y_pred3), ))\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_pred[i] = y_pred3[i]\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_true[i] = y_train[i]\n",
    "        trn_s = mean_absolute_error(y_true, y_pred)\n",
    "        logs['trn_score'] = trn_s\n",
    "        \n",
    "        X_val, y_val = self.data[1][0], self.data[1][1]\n",
    "        y_pred3 = self.model.predict(X_val)\n",
    "        y_pred = np.zeros((len(y_pred3), ))\n",
    "        y_true = np.zeros((len(y_pred3), ))\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_pred[i] = y_pred3[i]\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_true[i] = y_val[i]\n",
    "        val_s = mean_absolute_error(y_true, y_pred)\n",
    "        logs['val_score'] = val_s\n",
    "        print('trn_score', trn_s, 'val_score', val_s)\n",
    "\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T15:49:40.681988Z",
     "start_time": "2021-04-16T15:49:40.677491Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "  \n",
    "def scheduler(epoch):\n",
    "    # 每隔100个epoch，学习率减小为原来的1/10\n",
    "    if epoch % 20 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.6)\n",
    "        print(\"lr changed to {}\".format(lr * 0.6))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "reduce_lr = LearningRateScheduler(scheduler)\n",
    "#model.fit(train_x, train_y, batch_size=32, epochs=5, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:23:44.932490Z",
     "start_time": "2021-04-16T16:12:06.468160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2569.0168 - mae: 2569.0168 - val_loss: 904.7879 - val_mae: 904.7879\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 797.3423 - mae: 797.3423 - val_loss: 702.4642 - val_mae: 702.4642\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 678.4730 - mae: 678.4730 - val_loss: 608.1737 - val_mae: 608.1737\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 672.4891 - mae: 672.4891 - val_loss: 676.0610 - val_mae: 676.0610\n",
      "Epoch 5/145\n",
      "63/63 - 1s - loss: 569.9478 - mae: 569.9478 - val_loss: 536.1880 - val_mae: 536.1880\n",
      "Epoch 6/145\n",
      "63/63 - 1s - loss: 562.4293 - mae: 562.4293 - val_loss: 793.1977 - val_mae: 793.1977\n",
      "Epoch 7/145\n",
      "63/63 - 1s - loss: 562.5613 - mae: 562.5613 - val_loss: 513.3967 - val_mae: 513.3967\n",
      "Epoch 8/145\n",
      "63/63 - 1s - loss: 634.1051 - mae: 634.1051 - val_loss: 703.5747 - val_mae: 703.5747\n",
      "Epoch 9/145\n",
      "63/63 - 1s - loss: 544.2729 - mae: 544.2729 - val_loss: 570.0464 - val_mae: 570.0464\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 544.3489 - mae: 544.3489 - val_loss: 597.1138 - val_mae: 597.1138\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 544.9971 - mae: 544.9971 - val_loss: 500.5086 - val_mae: 500.5086\n",
      "Epoch 12/145\n",
      "63/63 - 1s - loss: 508.1191 - mae: 508.1192 - val_loss: 567.0406 - val_mae: 567.0406\n",
      "Epoch 13/145\n",
      "63/63 - 1s - loss: 530.2046 - mae: 530.2046 - val_loss: 509.0928 - val_mae: 509.0928\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 497.4692 - mae: 497.4692 - val_loss: 568.7343 - val_mae: 568.7343\n",
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 539.3836 - mae: 539.3836 - val_loss: 531.7739 - val_mae: 531.7739\n",
      "Epoch 16/145\n",
      "63/63 - 1s - loss: 482.5705 - mae: 482.5705 - val_loss: 530.4338 - val_mae: 530.4338\n",
      "Epoch 17/145\n",
      "63/63 - 1s - loss: 483.2744 - mae: 483.2744 - val_loss: 499.3446 - val_mae: 499.3446\n",
      "Epoch 18/145\n",
      "63/63 - 1s - loss: 481.6714 - mae: 481.6714 - val_loss: 519.0764 - val_mae: 519.0764\n",
      "Epoch 19/145\n",
      "63/63 - 1s - loss: 475.3432 - mae: 475.3432 - val_loss: 469.0109 - val_mae: 469.0109\n",
      "Epoch 20/145\n",
      "63/63 - 1s - loss: 500.2899 - mae: 500.2899 - val_loss: 483.4485 - val_mae: 483.4485\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 1s - loss: 447.1142 - mae: 447.1142 - val_loss: 457.0931 - val_mae: 457.0931\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 441.6724 - mae: 441.6724 - val_loss: 470.3813 - val_mae: 470.3813\n",
      "Epoch 23/145\n",
      "63/63 - 1s - loss: 446.5378 - mae: 446.5378 - val_loss: 482.7934 - val_mae: 482.7934\n",
      "Epoch 24/145\n",
      "63/63 - 1s - loss: 450.3069 - mae: 450.3069 - val_loss: 464.1917 - val_mae: 464.1917\n",
      "Epoch 25/145\n",
      "63/63 - 1s - loss: 443.2038 - mae: 443.2038 - val_loss: 457.6572 - val_mae: 457.6572\n",
      "Epoch 26/145\n",
      "63/63 - 1s - loss: 452.6797 - mae: 452.6797 - val_loss: 449.7480 - val_mae: 449.7480\n",
      "Epoch 27/145\n",
      "63/63 - 1s - loss: 438.3908 - mae: 438.3908 - val_loss: 449.9567 - val_mae: 449.9567\n",
      "Epoch 28/145\n",
      "63/63 - 1s - loss: 445.5518 - mae: 445.5518 - val_loss: 526.1703 - val_mae: 526.1703\n",
      "Epoch 29/145\n",
      "63/63 - 1s - loss: 465.5756 - mae: 465.5756 - val_loss: 463.8166 - val_mae: 463.8166\n",
      "Epoch 30/145\n",
      "63/63 - 1s - loss: 441.4665 - mae: 441.4665 - val_loss: 450.6729 - val_mae: 450.6729\n",
      "Epoch 31/145\n",
      "63/63 - 1s - loss: 453.7373 - mae: 453.7373 - val_loss: 465.4680 - val_mae: 465.4680\n",
      "Epoch 32/145\n",
      "63/63 - 1s - loss: 457.3779 - mae: 457.3779 - val_loss: 510.2656 - val_mae: 510.2656\n",
      "Epoch 33/145\n",
      "63/63 - 1s - loss: 474.1327 - mae: 474.1327 - val_loss: 471.4766 - val_mae: 471.4766\n",
      "Epoch 34/145\n",
      "63/63 - 1s - loss: 452.4405 - mae: 452.4405 - val_loss: 453.0840 - val_mae: 453.0840\n",
      "Epoch 35/145\n",
      "63/63 - 1s - loss: 456.5918 - mae: 456.5918 - val_loss: 504.1913 - val_mae: 504.1913\n",
      "Epoch 36/145\n",
      "63/63 - 1s - loss: 449.1606 - mae: 449.1606 - val_loss: 452.5743 - val_mae: 452.5743\n",
      "Epoch 37/145\n",
      "63/63 - 1s - loss: 452.6774 - mae: 452.6774 - val_loss: 475.9520 - val_mae: 475.9520\n",
      "Epoch 38/145\n",
      "63/63 - 1s - loss: 456.6142 - mae: 456.6142 - val_loss: 508.8349 - val_mae: 508.8349\n",
      "Epoch 39/145\n",
      "63/63 - 1s - loss: 443.2406 - mae: 443.2406 - val_loss: 458.6720 - val_mae: 458.6720\n",
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 428.6302 - mae: 428.6302 - val_loss: 501.3181 - val_mae: 501.3181\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 422.2794 - mae: 422.2794 - val_loss: 441.9762 - val_mae: 441.9762\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 411.6184 - mae: 411.6184 - val_loss: 437.8123 - val_mae: 437.8123\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 411.9478 - mae: 411.9478 - val_loss: 437.7370 - val_mae: 437.7370\n",
      "Epoch 44/145\n",
      "63/63 - 1s - loss: 409.1667 - mae: 409.1667 - val_loss: 434.8954 - val_mae: 434.8954\n",
      "Epoch 45/145\n",
      "63/63 - 1s - loss: 411.9498 - mae: 411.9498 - val_loss: 467.3405 - val_mae: 467.3405\n",
      "Epoch 46/145\n",
      "63/63 - 1s - loss: 418.3844 - mae: 418.3844 - val_loss: 437.9361 - val_mae: 437.9361\n",
      "Epoch 47/145\n",
      "63/63 - 1s - loss: 408.5166 - mae: 408.5166 - val_loss: 439.5873 - val_mae: 439.5873\n",
      "Epoch 48/145\n",
      "63/63 - 1s - loss: 406.6956 - mae: 406.6956 - val_loss: 437.0746 - val_mae: 437.0746\n",
      "Epoch 49/145\n",
      "63/63 - 1s - loss: 411.0063 - mae: 411.0063 - val_loss: 436.2401 - val_mae: 436.2401\n",
      "Epoch 50/145\n",
      "63/63 - 1s - loss: 407.8489 - mae: 407.8489 - val_loss: 468.8311 - val_mae: 468.8311\n",
      "Epoch 51/145\n",
      "63/63 - 1s - loss: 419.8079 - mae: 419.8079 - val_loss: 451.8352 - val_mae: 451.8352\n",
      "Epoch 52/145\n",
      "63/63 - 1s - loss: 405.4148 - mae: 405.4148 - val_loss: 431.4931 - val_mae: 431.4931\n",
      "Epoch 53/145\n",
      "63/63 - 1s - loss: 404.2062 - mae: 404.2062 - val_loss: 432.8908 - val_mae: 432.8908\n",
      "Epoch 54/145\n",
      "63/63 - 1s - loss: 402.2622 - mae: 402.2622 - val_loss: 435.1342 - val_mae: 435.1342\n",
      "Epoch 55/145\n",
      "63/63 - 1s - loss: 401.8087 - mae: 401.8087 - val_loss: 435.3172 - val_mae: 435.3172\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 411.3880 - mae: 411.3880 - val_loss: 479.4718 - val_mae: 479.4718\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 429.0826 - mae: 429.0826 - val_loss: 458.1339 - val_mae: 458.1339\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 422.6082 - mae: 422.6082 - val_loss: 450.3092 - val_mae: 450.3092\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 399.4478 - mae: 399.4478 - val_loss: 428.3436 - val_mae: 428.3436\n",
      "Epoch 60/145\n",
      "63/63 - 1s - loss: 395.9201 - mae: 395.9201 - val_loss: 432.8084 - val_mae: 432.8084\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 1s - loss: 390.1139 - mae: 390.1139 - val_loss: 425.6725 - val_mae: 425.6725\n",
      "Epoch 62/145\n",
      "63/63 - 1s - loss: 389.5950 - mae: 389.5950 - val_loss: 428.9451 - val_mae: 428.9451\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 390.6248 - mae: 390.6248 - val_loss: 422.8357 - val_mae: 422.8357\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 387.5200 - mae: 387.5200 - val_loss: 424.7711 - val_mae: 424.7711\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 388.2656 - mae: 388.2656 - val_loss: 426.3296 - val_mae: 426.3296\n",
      "Epoch 66/145\n",
      "63/63 - 1s - loss: 389.0033 - mae: 389.0033 - val_loss: 434.6791 - val_mae: 434.6791\n",
      "Epoch 67/145\n",
      "63/63 - 1s - loss: 386.8221 - mae: 386.8221 - val_loss: 427.8208 - val_mae: 427.8208\n",
      "Epoch 68/145\n",
      "63/63 - 1s - loss: 387.0233 - mae: 387.0233 - val_loss: 423.7752 - val_mae: 423.7752\n",
      "Epoch 69/145\n",
      "63/63 - 1s - loss: 385.5024 - mae: 385.5024 - val_loss: 423.6107 - val_mae: 423.6107\n",
      "Epoch 70/145\n",
      "63/63 - 1s - loss: 384.5536 - mae: 384.5536 - val_loss: 421.6582 - val_mae: 421.6582\n",
      "Epoch 71/145\n",
      "63/63 - 1s - loss: 385.6678 - mae: 385.6678 - val_loss: 423.3311 - val_mae: 423.3311\n",
      "Epoch 72/145\n",
      "63/63 - 1s - loss: 386.1680 - mae: 386.1680 - val_loss: 429.2517 - val_mae: 429.2517\n",
      "Epoch 73/145\n",
      "63/63 - 1s - loss: 393.1091 - mae: 393.1091 - val_loss: 427.2532 - val_mae: 427.2532\n",
      "Epoch 74/145\n",
      "63/63 - 1s - loss: 384.9462 - mae: 384.9462 - val_loss: 423.2542 - val_mae: 423.2542\n",
      "Epoch 75/145\n",
      "63/63 - 1s - loss: 381.5010 - mae: 381.5010 - val_loss: 422.2299 - val_mae: 422.2299\n",
      "Epoch 76/145\n",
      "63/63 - 1s - loss: 381.7902 - mae: 381.7902 - val_loss: 425.9357 - val_mae: 425.9357\n",
      "Epoch 77/145\n",
      "63/63 - 1s - loss: 382.3887 - mae: 382.3887 - val_loss: 430.9576 - val_mae: 430.9576\n",
      "Epoch 78/145\n",
      "63/63 - 1s - loss: 382.5799 - mae: 382.5799 - val_loss: 420.5164 - val_mae: 420.5164\n",
      "Epoch 79/145\n",
      "63/63 - 1s - loss: 380.4527 - mae: 380.4527 - val_loss: 426.4119 - val_mae: 426.4119\n",
      "Epoch 80/145\n",
      "63/63 - 1s - loss: 388.6375 - mae: 388.6375 - val_loss: 422.4250 - val_mae: 422.4250\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 1s - loss: 376.4361 - mae: 376.4361 - val_loss: 419.1861 - val_mae: 419.1861\n",
      "Epoch 82/145\n",
      "63/63 - 1s - loss: 374.1615 - mae: 374.1615 - val_loss: 421.1205 - val_mae: 421.1205\n",
      "Epoch 83/145\n",
      "63/63 - 1s - loss: 374.5322 - mae: 374.5322 - val_loss: 420.7596 - val_mae: 420.7596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/145\n",
      "63/63 - 1s - loss: 374.7967 - mae: 374.7967 - val_loss: 421.0315 - val_mae: 421.0315\n",
      "Epoch 85/145\n",
      "63/63 - 1s - loss: 373.9765 - mae: 373.9765 - val_loss: 420.3019 - val_mae: 420.3019\n",
      "Epoch 86/145\n",
      "63/63 - 1s - loss: 373.0105 - mae: 373.0105 - val_loss: 420.4583 - val_mae: 420.4583\n",
      "Epoch 87/145\n",
      "63/63 - 1s - loss: 373.1844 - mae: 373.1844 - val_loss: 420.6867 - val_mae: 420.6867\n",
      "Epoch 88/145\n",
      "63/63 - 1s - loss: 373.8559 - mae: 373.8559 - val_loss: 422.5433 - val_mae: 422.5433\n",
      "Epoch 89/145\n",
      "63/63 - 1s - loss: 372.9210 - mae: 372.9210 - val_loss: 420.4937 - val_mae: 420.4937\n",
      "Epoch 90/145\n",
      "63/63 - 1s - loss: 373.0705 - mae: 373.0705 - val_loss: 427.4324 - val_mae: 427.4324\n",
      "Epoch 91/145\n",
      "63/63 - 1s - loss: 373.0430 - mae: 373.0430 - val_loss: 418.8100 - val_mae: 418.8100\n",
      "Epoch 92/145\n",
      "63/63 - 1s - loss: 371.2543 - mae: 371.2543 - val_loss: 426.1605 - val_mae: 426.1605\n",
      "Epoch 93/145\n",
      "63/63 - 1s - loss: 373.4494 - mae: 373.4494 - val_loss: 431.3881 - val_mae: 431.3881\n",
      "Epoch 94/145\n",
      "63/63 - 1s - loss: 374.2716 - mae: 374.2716 - val_loss: 426.9983 - val_mae: 426.9983\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 372.3983 - mae: 372.3983 - val_loss: 421.1272 - val_mae: 421.1272\n",
      "Epoch 96/145\n",
      "63/63 - 1s - loss: 369.7102 - mae: 369.7102 - val_loss: 422.8129 - val_mae: 422.8129\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 370.3880 - mae: 370.3880 - val_loss: 419.8561 - val_mae: 419.8561\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 369.4482 - mae: 369.4482 - val_loss: 420.5718 - val_mae: 420.5718\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 369.8276 - mae: 369.8276 - val_loss: 420.6286 - val_mae: 420.6286\n",
      "Epoch 100/145\n",
      "63/63 - 1s - loss: 369.3513 - mae: 369.3513 - val_loss: 421.8488 - val_mae: 421.8488\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 1s - loss: 366.7921 - mae: 366.7921 - val_loss: 422.2633 - val_mae: 422.2633\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 366.6180 - mae: 366.6180 - val_loss: 422.2304 - val_mae: 422.2304\n",
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 364.8093 - mae: 364.8093 - val_loss: 421.4973 - val_mae: 421.4973\n",
      "Epoch 104/145\n",
      "63/63 - 1s - loss: 364.8589 - mae: 364.8589 - val_loss: 419.4527 - val_mae: 419.4527\n",
      "Epoch 105/145\n",
      "63/63 - 1s - loss: 363.9432 - mae: 363.9432 - val_loss: 421.1175 - val_mae: 421.1175\n",
      "Epoch 106/145\n",
      "63/63 - 1s - loss: 364.1917 - mae: 364.1917 - val_loss: 417.8030 - val_mae: 417.8030\n",
      "Epoch 107/145\n",
      "63/63 - 1s - loss: 363.7912 - mae: 363.7912 - val_loss: 421.1797 - val_mae: 421.1797\n",
      "Epoch 108/145\n",
      "63/63 - 1s - loss: 364.4863 - mae: 364.4863 - val_loss: 418.4117 - val_mae: 418.4117\n",
      "Epoch 109/145\n",
      "63/63 - 1s - loss: 363.2320 - mae: 363.2320 - val_loss: 419.6023 - val_mae: 419.6023\n",
      "Epoch 110/145\n",
      "63/63 - 1s - loss: 363.0803 - mae: 363.0803 - val_loss: 421.2479 - val_mae: 421.2479\n",
      "Epoch 111/145\n",
      "63/63 - 1s - loss: 363.4674 - mae: 363.4674 - val_loss: 421.5569 - val_mae: 421.5569\n",
      "Epoch 112/145\n",
      "63/63 - 1s - loss: 364.2495 - mae: 364.2495 - val_loss: 419.1096 - val_mae: 419.1096\n",
      "Epoch 113/145\n",
      "63/63 - 1s - loss: 362.5958 - mae: 362.5958 - val_loss: 419.4531 - val_mae: 419.4531\n",
      "Epoch 114/145\n",
      "63/63 - 1s - loss: 361.6780 - mae: 361.6780 - val_loss: 420.0953 - val_mae: 420.0953\n",
      "Epoch 115/145\n",
      "63/63 - 1s - loss: 361.6820 - mae: 361.6820 - val_loss: 422.8367 - val_mae: 422.8367\n",
      "Epoch 116/145\n",
      "63/63 - 1s - loss: 362.1413 - mae: 362.1413 - val_loss: 420.6770 - val_mae: 420.6770\n",
      "Epoch 117/145\n",
      "63/63 - 1s - loss: 361.1719 - mae: 361.1719 - val_loss: 419.5959 - val_mae: 419.5959\n",
      "Epoch 118/145\n",
      "63/63 - 1s - loss: 361.1246 - mae: 361.1246 - val_loss: 420.6785 - val_mae: 420.6785\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 360.9933 - mae: 360.9933 - val_loss: 421.8386 - val_mae: 421.8386\n",
      "Epoch 120/145\n",
      "63/63 - 1s - loss: 362.6567 - mae: 362.6567 - val_loss: 420.4167 - val_mae: 420.4167\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 358.8632 - mae: 358.8632 - val_loss: 418.9054 - val_mae: 418.9054\n",
      "Epoch 122/145\n",
      "63/63 - 1s - loss: 358.5045 - mae: 358.5045 - val_loss: 419.8524 - val_mae: 419.8524\n",
      "Epoch 123/145\n",
      "63/63 - 1s - loss: 358.4346 - mae: 358.4346 - val_loss: 419.1443 - val_mae: 419.1443\n",
      "Epoch 124/145\n",
      "63/63 - 1s - loss: 357.8319 - mae: 357.8319 - val_loss: 420.9480 - val_mae: 420.9480\n",
      "Epoch 125/145\n",
      "63/63 - 1s - loss: 358.1297 - mae: 358.1297 - val_loss: 418.8186 - val_mae: 418.8186\n",
      "Epoch 126/145\n",
      "63/63 - 1s - loss: 357.5340 - mae: 357.5340 - val_loss: 420.2756 - val_mae: 420.2756\n",
      "Epoch 127/145\n",
      "63/63 - 1s - loss: 357.7788 - mae: 357.7788 - val_loss: 419.4023 - val_mae: 419.4023\n",
      "Epoch 128/145\n",
      "63/63 - 1s - loss: 357.4367 - mae: 357.4367 - val_loss: 420.7859 - val_mae: 420.7859\n",
      "Epoch 129/145\n",
      "63/63 - 1s - loss: 357.6144 - mae: 357.6144 - val_loss: 419.4828 - val_mae: 419.4828\n",
      "Epoch 130/145\n",
      "63/63 - 1s - loss: 357.3270 - mae: 357.3270 - val_loss: 419.9085 - val_mae: 419.9085\n",
      "Epoch 131/145\n",
      "63/63 - 1s - loss: 357.0692 - mae: 357.0692 - val_loss: 420.7916 - val_mae: 420.7916\n",
      "Epoch 132/145\n",
      "63/63 - 1s - loss: 357.2558 - mae: 357.2558 - val_loss: 419.7760 - val_mae: 419.7760\n",
      "Epoch 133/145\n",
      "63/63 - 1s - loss: 356.7630 - mae: 356.7630 - val_loss: 418.4161 - val_mae: 418.4161\n",
      "Epoch 134/145\n",
      "63/63 - 1s - loss: 356.2641 - mae: 356.2641 - val_loss: 418.8596 - val_mae: 418.8596\n",
      "Epoch 135/145\n",
      "63/63 - 1s - loss: 356.2278 - mae: 356.2278 - val_loss: 418.9396 - val_mae: 418.9396\n",
      "Epoch 136/145\n",
      "63/63 - 1s - loss: 356.3503 - mae: 356.3503 - val_loss: 419.4193 - val_mae: 419.4193\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 355.3875 - mae: 355.3875 - val_loss: 418.8027 - val_mae: 418.8027\n",
      "Epoch 138/145\n",
      "63/63 - 1s - loss: 355.6281 - mae: 355.6281 - val_loss: 420.7141 - val_mae: 420.7141\n",
      "Epoch 139/145\n",
      "63/63 - 1s - loss: 355.6314 - mae: 355.6314 - val_loss: 420.6087 - val_mae: 420.6087\n",
      "Epoch 140/145\n",
      "63/63 - 1s - loss: 355.3333 - mae: 355.3333 - val_loss: 419.6113 - val_mae: 419.6113\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 354.0706 - mae: 354.0706 - val_loss: 419.8118 - val_mae: 419.8118\n",
      "Epoch 142/145\n",
      "63/63 - 1s - loss: 353.6299 - mae: 353.6299 - val_loss: 419.7306 - val_mae: 419.7306\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 353.6977 - mae: 353.6977 - val_loss: 420.3719 - val_mae: 420.3719\n",
      "Epoch 144/145\n",
      "63/63 - 1s - loss: 353.7716 - mae: 353.7716 - val_loss: 419.3142 - val_mae: 419.3142\n",
      "Epoch 145/145\n",
      "63/63 - 1s - loss: 353.2749 - mae: 353.2749 - val_loss: 420.0466 - val_mae: 420.0466\n",
      "\n",
      "val_mae is:420.04654629751207\n",
      "\n",
      "fold: 1\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2754.9001 - mae: 2754.9001 - val_loss: 971.2634 - val_mae: 971.2634\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 824.6105 - mae: 824.6105 - val_loss: 710.1059 - val_mae: 710.1059\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 721.4519 - mae: 721.4519 - val_loss: 638.1782 - val_mae: 638.1782\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 637.5015 - mae: 637.5015 - val_loss: 695.4756 - val_mae: 695.4756\n",
      "Epoch 5/145\n",
      "63/63 - 1s - loss: 606.4071 - mae: 606.4071 - val_loss: 592.2428 - val_mae: 592.2428\n",
      "Epoch 6/145\n",
      "63/63 - 1s - loss: 564.6874 - mae: 564.6874 - val_loss: 562.2194 - val_mae: 562.2194\n",
      "Epoch 7/145\n",
      "63/63 - 1s - loss: 546.7816 - mae: 546.7816 - val_loss: 540.0671 - val_mae: 540.0671\n",
      "Epoch 8/145\n",
      "63/63 - 1s - loss: 535.2396 - mae: 535.2396 - val_loss: 552.0170 - val_mae: 552.0170\n",
      "Epoch 9/145\n",
      "63/63 - 1s - loss: 529.5869 - mae: 529.5869 - val_loss: 541.0120 - val_mae: 541.0120\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 519.4591 - mae: 519.4591 - val_loss: 510.9538 - val_mae: 510.9538\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 509.1793 - mae: 509.1793 - val_loss: 535.3152 - val_mae: 535.3152\n",
      "Epoch 12/145\n",
      "63/63 - 1s - loss: 553.6117 - mae: 553.6117 - val_loss: 607.8867 - val_mae: 607.8867\n",
      "Epoch 13/145\n",
      "63/63 - 1s - loss: 542.7914 - mae: 542.7914 - val_loss: 569.0074 - val_mae: 569.0074\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 522.0152 - mae: 522.0152 - val_loss: 499.2280 - val_mae: 499.2280\n",
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 485.9939 - mae: 485.9939 - val_loss: 503.4114 - val_mae: 503.4114\n",
      "Epoch 16/145\n",
      "63/63 - 1s - loss: 515.0471 - mae: 515.0470 - val_loss: 585.9951 - val_mae: 585.9951\n",
      "Epoch 17/145\n",
      "63/63 - 1s - loss: 503.6520 - mae: 503.6520 - val_loss: 501.7185 - val_mae: 501.7185\n",
      "Epoch 18/145\n",
      "63/63 - 1s - loss: 473.8903 - mae: 473.8903 - val_loss: 469.3468 - val_mae: 469.3468\n",
      "Epoch 19/145\n",
      "63/63 - 1s - loss: 473.2816 - mae: 473.2816 - val_loss: 475.8302 - val_mae: 475.8302\n",
      "Epoch 20/145\n",
      "63/63 - 1s - loss: 465.2918 - mae: 465.2918 - val_loss: 466.4561 - val_mae: 466.4561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 1s - loss: 446.3428 - mae: 446.3428 - val_loss: 493.9195 - val_mae: 493.9195\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 470.1374 - mae: 470.1374 - val_loss: 454.8164 - val_mae: 454.8164\n",
      "Epoch 23/145\n",
      "63/63 - 1s - loss: 444.0622 - mae: 444.0622 - val_loss: 471.0299 - val_mae: 471.0299\n",
      "Epoch 24/145\n",
      "63/63 - 1s - loss: 439.6766 - mae: 439.6766 - val_loss: 449.0094 - val_mae: 449.0094\n",
      "Epoch 25/145\n",
      "63/63 - 1s - loss: 439.9623 - mae: 439.9623 - val_loss: 452.9448 - val_mae: 452.9448\n",
      "Epoch 26/145\n",
      "63/63 - 1s - loss: 434.9466 - mae: 434.9466 - val_loss: 451.9504 - val_mae: 451.9504\n",
      "Epoch 27/145\n",
      "63/63 - 1s - loss: 438.5275 - mae: 438.5275 - val_loss: 447.3272 - val_mae: 447.3272\n",
      "Epoch 28/145\n",
      "63/63 - 1s - loss: 466.4695 - mae: 466.4695 - val_loss: 472.4979 - val_mae: 472.4979\n",
      "Epoch 29/145\n",
      "63/63 - 1s - loss: 457.0103 - mae: 457.0103 - val_loss: 467.0770 - val_mae: 467.0770\n",
      "Epoch 30/145\n",
      "63/63 - 1s - loss: 433.0109 - mae: 433.0109 - val_loss: 443.8711 - val_mae: 443.8711\n",
      "Epoch 31/145\n",
      "63/63 - 1s - loss: 444.1230 - mae: 444.1230 - val_loss: 443.2117 - val_mae: 443.2117\n",
      "Epoch 32/145\n",
      "63/63 - 1s - loss: 432.2425 - mae: 432.2425 - val_loss: 443.4136 - val_mae: 443.4136\n",
      "Epoch 33/145\n",
      "63/63 - 1s - loss: 427.4978 - mae: 427.4978 - val_loss: 471.9165 - val_mae: 471.9165\n",
      "Epoch 34/145\n",
      "63/63 - 1s - loss: 430.6549 - mae: 430.6549 - val_loss: 441.6656 - val_mae: 441.6656\n",
      "Epoch 35/145\n",
      "63/63 - 1s - loss: 428.5737 - mae: 428.5737 - val_loss: 445.3167 - val_mae: 445.3167\n",
      "Epoch 36/145\n",
      "63/63 - 1s - loss: 423.8148 - mae: 423.8148 - val_loss: 447.8394 - val_mae: 447.8394\n",
      "Epoch 37/145\n",
      "63/63 - 1s - loss: 458.9902 - mae: 458.9902 - val_loss: 492.7330 - val_mae: 492.7330\n",
      "Epoch 38/145\n",
      "63/63 - 1s - loss: 433.8402 - mae: 433.8402 - val_loss: 439.4984 - val_mae: 439.4984\n",
      "Epoch 39/145\n",
      "63/63 - 1s - loss: 434.1112 - mae: 434.1112 - val_loss: 441.8071 - val_mae: 441.8071\n",
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 421.1402 - mae: 421.1402 - val_loss: 444.7758 - val_mae: 444.7758\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 412.6790 - mae: 412.6790 - val_loss: 432.7830 - val_mae: 432.7830\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 414.7014 - mae: 414.7014 - val_loss: 432.1146 - val_mae: 432.1146\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 408.8973 - mae: 408.8973 - val_loss: 433.4570 - val_mae: 433.4570\n",
      "Epoch 44/145\n",
      "63/63 - 1s - loss: 410.6755 - mae: 410.6755 - val_loss: 441.4918 - val_mae: 441.4918\n",
      "Epoch 45/145\n",
      "63/63 - 1s - loss: 408.3818 - mae: 408.3818 - val_loss: 432.6813 - val_mae: 432.6813\n",
      "Epoch 46/145\n",
      "63/63 - 1s - loss: 406.6220 - mae: 406.6220 - val_loss: 430.8081 - val_mae: 430.8081\n",
      "Epoch 47/145\n",
      "63/63 - 1s - loss: 407.8168 - mae: 407.8168 - val_loss: 431.1245 - val_mae: 431.1245\n",
      "Epoch 48/145\n",
      "63/63 - 1s - loss: 407.1781 - mae: 407.1781 - val_loss: 448.0145 - val_mae: 448.0145\n",
      "Epoch 49/145\n",
      "63/63 - 1s - loss: 404.5660 - mae: 404.5660 - val_loss: 430.1518 - val_mae: 430.1518\n",
      "Epoch 50/145\n",
      "63/63 - 1s - loss: 405.9767 - mae: 405.9767 - val_loss: 452.2634 - val_mae: 452.2634\n",
      "Epoch 51/145\n",
      "63/63 - 1s - loss: 407.5479 - mae: 407.5479 - val_loss: 432.3799 - val_mae: 432.3799\n",
      "Epoch 52/145\n",
      "63/63 - 1s - loss: 404.2224 - mae: 404.2224 - val_loss: 437.5377 - val_mae: 437.5377\n",
      "Epoch 53/145\n",
      "63/63 - 1s - loss: 411.5182 - mae: 411.5182 - val_loss: 430.5151 - val_mae: 430.5151\n",
      "Epoch 54/145\n",
      "63/63 - 1s - loss: 404.8965 - mae: 404.8965 - val_loss: 430.0473 - val_mae: 430.0473\n",
      "Epoch 55/145\n",
      "63/63 - 1s - loss: 400.5999 - mae: 400.5999 - val_loss: 433.3502 - val_mae: 433.3502\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 402.5225 - mae: 402.5225 - val_loss: 432.4080 - val_mae: 432.4080\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 400.4143 - mae: 400.4143 - val_loss: 432.6862 - val_mae: 432.6862\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 399.3155 - mae: 399.3155 - val_loss: 425.2321 - val_mae: 425.2321\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 401.6906 - mae: 401.6906 - val_loss: 426.1833 - val_mae: 426.1833\n",
      "Epoch 60/145\n",
      "63/63 - 1s - loss: 401.1970 - mae: 401.1970 - val_loss: 424.8785 - val_mae: 424.8785\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 1s - loss: 392.4302 - mae: 392.4302 - val_loss: 425.5912 - val_mae: 425.5912\n",
      "Epoch 62/145\n",
      "63/63 - 1s - loss: 391.2883 - mae: 391.2883 - val_loss: 423.1331 - val_mae: 423.1331\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 391.2039 - mae: 391.2039 - val_loss: 422.2990 - val_mae: 422.2990\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 391.1745 - mae: 391.1745 - val_loss: 422.6237 - val_mae: 422.6237\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 391.5935 - mae: 391.5935 - val_loss: 422.1330 - val_mae: 422.1330\n",
      "Epoch 66/145\n",
      "63/63 - 1s - loss: 390.8723 - mae: 390.8723 - val_loss: 421.3298 - val_mae: 421.3298\n",
      "Epoch 67/145\n",
      "63/63 - 1s - loss: 390.2521 - mae: 390.2521 - val_loss: 422.3842 - val_mae: 422.3842\n",
      "Epoch 68/145\n",
      "63/63 - 1s - loss: 389.2458 - mae: 389.2458 - val_loss: 422.0086 - val_mae: 422.0086\n",
      "Epoch 69/145\n",
      "63/63 - 1s - loss: 388.9790 - mae: 388.9790 - val_loss: 420.3881 - val_mae: 420.3881\n",
      "Epoch 70/145\n",
      "63/63 - 1s - loss: 387.9912 - mae: 387.9912 - val_loss: 420.1615 - val_mae: 420.1615\n",
      "Epoch 71/145\n",
      "63/63 - 1s - loss: 387.1267 - mae: 387.1267 - val_loss: 422.1017 - val_mae: 422.1017\n",
      "Epoch 72/145\n",
      "63/63 - 1s - loss: 391.4980 - mae: 391.4980 - val_loss: 422.2856 - val_mae: 422.2856\n",
      "Epoch 73/145\n",
      "63/63 - 1s - loss: 386.8611 - mae: 386.8611 - val_loss: 434.4278 - val_mae: 434.4278\n",
      "Epoch 74/145\n",
      "63/63 - 1s - loss: 389.6158 - mae: 389.6158 - val_loss: 426.2275 - val_mae: 426.2275\n",
      "Epoch 75/145\n",
      "63/63 - 1s - loss: 386.5431 - mae: 386.5431 - val_loss: 424.3532 - val_mae: 424.3532\n",
      "Epoch 76/145\n",
      "63/63 - 1s - loss: 386.3641 - mae: 386.3641 - val_loss: 421.8322 - val_mae: 421.8322\n",
      "Epoch 77/145\n",
      "63/63 - 1s - loss: 386.3765 - mae: 386.3765 - val_loss: 422.9285 - val_mae: 422.9285\n",
      "Epoch 78/145\n",
      "63/63 - 1s - loss: 385.7222 - mae: 385.7222 - val_loss: 423.4773 - val_mae: 423.4773\n",
      "Epoch 79/145\n",
      "63/63 - 1s - loss: 384.4301 - mae: 384.4301 - val_loss: 424.7064 - val_mae: 424.7064\n",
      "Epoch 80/145\n",
      "63/63 - 1s - loss: 385.4035 - mae: 385.4035 - val_loss: 432.2216 - val_mae: 432.2216\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 1s - loss: 382.7606 - mae: 382.7606 - val_loss: 418.0104 - val_mae: 418.0104\n",
      "Epoch 82/145\n",
      "63/63 - 1s - loss: 379.3272 - mae: 379.3272 - val_loss: 418.7278 - val_mae: 418.7278\n",
      "Epoch 83/145\n",
      "63/63 - 1s - loss: 379.6331 - mae: 379.6331 - val_loss: 420.9523 - val_mae: 420.9523\n",
      "Epoch 84/145\n",
      "63/63 - 1s - loss: 379.3058 - mae: 379.3058 - val_loss: 422.2518 - val_mae: 422.2518\n",
      "Epoch 85/145\n",
      "63/63 - 1s - loss: 379.6431 - mae: 379.6431 - val_loss: 423.7814 - val_mae: 423.7814\n",
      "Epoch 86/145\n",
      "63/63 - 1s - loss: 378.5058 - mae: 378.5058 - val_loss: 417.5725 - val_mae: 417.5725\n",
      "Epoch 87/145\n",
      "63/63 - 1s - loss: 378.0493 - mae: 378.0493 - val_loss: 417.8838 - val_mae: 417.8838\n",
      "Epoch 88/145\n",
      "63/63 - 1s - loss: 377.9585 - mae: 377.9585 - val_loss: 418.5270 - val_mae: 418.5270\n",
      "Epoch 89/145\n",
      "63/63 - 1s - loss: 377.5909 - mae: 377.5909 - val_loss: 416.9282 - val_mae: 416.9282\n",
      "Epoch 90/145\n",
      "63/63 - 1s - loss: 377.8695 - mae: 377.8695 - val_loss: 421.2776 - val_mae: 421.2776\n",
      "Epoch 91/145\n",
      "63/63 - 1s - loss: 377.9521 - mae: 377.9521 - val_loss: 421.0875 - val_mae: 421.0875\n",
      "Epoch 92/145\n",
      "63/63 - 1s - loss: 377.0834 - mae: 377.0834 - val_loss: 417.5201 - val_mae: 417.5201\n",
      "Epoch 93/145\n",
      "63/63 - 1s - loss: 376.3650 - mae: 376.3650 - val_loss: 418.5955 - val_mae: 418.5955\n",
      "Epoch 94/145\n",
      "63/63 - 1s - loss: 376.5663 - mae: 376.5663 - val_loss: 419.7770 - val_mae: 419.7770\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 375.5593 - mae: 375.5593 - val_loss: 417.4477 - val_mae: 417.4477\n",
      "Epoch 96/145\n",
      "63/63 - 1s - loss: 375.0074 - mae: 375.0074 - val_loss: 418.1064 - val_mae: 418.1064\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 376.6935 - mae: 376.6935 - val_loss: 420.2849 - val_mae: 420.2849\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 375.8846 - mae: 375.8846 - val_loss: 417.9095 - val_mae: 417.9095\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 374.9203 - mae: 374.9203 - val_loss: 420.4004 - val_mae: 420.4004\n",
      "Epoch 100/145\n",
      "63/63 - 1s - loss: 374.7197 - mae: 374.7197 - val_loss: 418.0741 - val_mae: 418.0741\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 1s - loss: 372.5233 - mae: 372.5233 - val_loss: 415.7169 - val_mae: 415.7169\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 371.5605 - mae: 371.5605 - val_loss: 415.9471 - val_mae: 415.9471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 371.7419 - mae: 371.7419 - val_loss: 416.1692 - val_mae: 416.1692\n",
      "Epoch 104/145\n",
      "63/63 - 1s - loss: 371.8199 - mae: 371.8199 - val_loss: 414.7339 - val_mae: 414.7339\n",
      "Epoch 105/145\n",
      "63/63 - 1s - loss: 370.6423 - mae: 370.6423 - val_loss: 416.0244 - val_mae: 416.0244\n",
      "Epoch 106/145\n",
      "63/63 - 1s - loss: 370.9224 - mae: 370.9224 - val_loss: 416.5233 - val_mae: 416.5233\n",
      "Epoch 107/145\n",
      "63/63 - 1s - loss: 370.5094 - mae: 370.5094 - val_loss: 415.2392 - val_mae: 415.2392\n",
      "Epoch 108/145\n",
      "63/63 - 1s - loss: 370.6679 - mae: 370.6679 - val_loss: 416.3966 - val_mae: 416.3966\n",
      "Epoch 109/145\n",
      "63/63 - 1s - loss: 370.8288 - mae: 370.8288 - val_loss: 415.1439 - val_mae: 415.1439\n",
      "Epoch 110/145\n",
      "63/63 - 1s - loss: 369.5905 - mae: 369.5905 - val_loss: 415.3889 - val_mae: 415.3889\n",
      "Epoch 111/145\n",
      "63/63 - 1s - loss: 369.5031 - mae: 369.5031 - val_loss: 415.9321 - val_mae: 415.9321\n",
      "Epoch 112/145\n",
      "63/63 - 1s - loss: 369.2960 - mae: 369.2960 - val_loss: 415.2680 - val_mae: 415.2680\n",
      "Epoch 113/145\n",
      "63/63 - 1s - loss: 369.2232 - mae: 369.2232 - val_loss: 415.5902 - val_mae: 415.5902\n",
      "Epoch 114/145\n",
      "63/63 - 1s - loss: 368.5520 - mae: 368.5520 - val_loss: 415.6123 - val_mae: 415.6123\n",
      "Epoch 115/145\n",
      "63/63 - 1s - loss: 368.6082 - mae: 368.6082 - val_loss: 414.2900 - val_mae: 414.2900\n",
      "Epoch 116/145\n",
      "63/63 - 1s - loss: 368.9523 - mae: 368.9523 - val_loss: 414.5775 - val_mae: 414.5775\n",
      "Epoch 117/145\n",
      "63/63 - 1s - loss: 369.3676 - mae: 369.3676 - val_loss: 416.2266 - val_mae: 416.2266\n",
      "Epoch 118/145\n",
      "63/63 - 1s - loss: 368.0836 - mae: 368.0836 - val_loss: 414.9086 - val_mae: 414.9086\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 368.2856 - mae: 368.2856 - val_loss: 415.8793 - val_mae: 415.8793\n",
      "Epoch 120/145\n",
      "63/63 - 1s - loss: 368.9340 - mae: 368.9340 - val_loss: 415.3293 - val_mae: 415.3293\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 366.8430 - mae: 366.8430 - val_loss: 414.5400 - val_mae: 414.5400\n",
      "Epoch 122/145\n",
      "63/63 - 1s - loss: 366.6680 - mae: 366.6680 - val_loss: 415.4318 - val_mae: 415.4318\n",
      "Epoch 123/145\n",
      "63/63 - 1s - loss: 365.8148 - mae: 365.8148 - val_loss: 414.4254 - val_mae: 414.4254\n",
      "Epoch 124/145\n",
      "63/63 - 1s - loss: 365.9557 - mae: 365.9557 - val_loss: 414.2133 - val_mae: 414.2133\n",
      "Epoch 125/145\n",
      "63/63 - 1s - loss: 365.4527 - mae: 365.4527 - val_loss: 414.2969 - val_mae: 414.2969\n",
      "Epoch 126/145\n",
      "63/63 - 1s - loss: 365.6577 - mae: 365.6577 - val_loss: 415.4177 - val_mae: 415.4177\n",
      "Epoch 127/145\n",
      "63/63 - 1s - loss: 365.6737 - mae: 365.6737 - val_loss: 414.4852 - val_mae: 414.4852\n",
      "Epoch 128/145\n",
      "63/63 - 1s - loss: 365.6851 - mae: 365.6851 - val_loss: 417.4727 - val_mae: 417.4727\n",
      "Epoch 129/145\n",
      "63/63 - 1s - loss: 365.0821 - mae: 365.0821 - val_loss: 414.9396 - val_mae: 414.9396\n",
      "Epoch 130/145\n",
      "63/63 - 1s - loss: 365.0668 - mae: 365.0668 - val_loss: 413.9890 - val_mae: 413.9890\n",
      "Epoch 131/145\n",
      "63/63 - 1s - loss: 364.7315 - mae: 364.7315 - val_loss: 414.1759 - val_mae: 414.1759\n",
      "Epoch 132/145\n",
      "63/63 - 1s - loss: 364.9315 - mae: 364.9315 - val_loss: 414.0931 - val_mae: 414.0931\n",
      "Epoch 133/145\n",
      "63/63 - 1s - loss: 364.5450 - mae: 364.5450 - val_loss: 414.0295 - val_mae: 414.0295\n",
      "Epoch 134/145\n",
      "63/63 - 1s - loss: 364.4173 - mae: 364.4173 - val_loss: 414.2737 - val_mae: 414.2737\n",
      "Epoch 135/145\n",
      "63/63 - 1s - loss: 364.4690 - mae: 364.4690 - val_loss: 413.8808 - val_mae: 413.8808\n",
      "Epoch 136/145\n",
      "63/63 - 1s - loss: 364.3720 - mae: 364.3720 - val_loss: 414.2137 - val_mae: 414.2137\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 364.4617 - mae: 364.4617 - val_loss: 414.0846 - val_mae: 414.0846\n",
      "Epoch 138/145\n",
      "63/63 - 1s - loss: 363.8850 - mae: 363.8850 - val_loss: 413.9391 - val_mae: 413.9391\n",
      "Epoch 139/145\n",
      "63/63 - 1s - loss: 364.0937 - mae: 364.0937 - val_loss: 415.8232 - val_mae: 415.8232\n",
      "Epoch 140/145\n",
      "63/63 - 1s - loss: 364.0196 - mae: 364.0196 - val_loss: 415.1491 - val_mae: 415.1491\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 362.8608 - mae: 362.8608 - val_loss: 413.7370 - val_mae: 413.7370\n",
      "Epoch 142/145\n",
      "63/63 - 1s - loss: 362.6580 - mae: 362.6580 - val_loss: 413.8466 - val_mae: 413.8466\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 362.6887 - mae: 362.6887 - val_loss: 414.0142 - val_mae: 414.0142\n",
      "Epoch 144/145\n",
      "63/63 - 1s - loss: 362.2121 - mae: 362.2121 - val_loss: 413.9812 - val_mae: 413.9812\n",
      "Epoch 145/145\n",
      "63/63 - 1s - loss: 362.3738 - mae: 362.3738 - val_loss: 413.9732 - val_mae: 413.9732\n",
      "\n",
      "val_mae is:413.97315094117164\n",
      "\n",
      "fold: 2\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2537.7068 - mae: 2537.7068 - val_loss: 929.7588 - val_mae: 929.7588\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 775.2897 - mae: 775.2897 - val_loss: 710.2006 - val_mae: 710.2006\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 712.9670 - mae: 712.9670 - val_loss: 715.6061 - val_mae: 715.6061\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 672.9084 - mae: 672.9084 - val_loss: 608.3923 - val_mae: 608.3923\n",
      "Epoch 5/145\n",
      "63/63 - 1s - loss: 579.2977 - mae: 579.2977 - val_loss: 568.8353 - val_mae: 568.8353\n",
      "Epoch 6/145\n",
      "63/63 - 1s - loss: 581.5365 - mae: 581.5365 - val_loss: 650.3460 - val_mae: 650.3460\n",
      "Epoch 7/145\n",
      "63/63 - 1s - loss: 566.4731 - mae: 566.4731 - val_loss: 554.4661 - val_mae: 554.4661\n",
      "Epoch 8/145\n",
      "63/63 - 1s - loss: 533.5838 - mae: 533.5838 - val_loss: 512.0026 - val_mae: 512.0026\n",
      "Epoch 9/145\n",
      "63/63 - 1s - loss: 526.9946 - mae: 526.9946 - val_loss: 614.8982 - val_mae: 614.8982\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 608.7753 - mae: 608.7753 - val_loss: 674.0232 - val_mae: 674.0232\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 552.5053 - mae: 552.5053 - val_loss: 555.8891 - val_mae: 555.8891\n",
      "Epoch 12/145\n",
      "63/63 - 1s - loss: 499.9044 - mae: 499.9044 - val_loss: 527.3496 - val_mae: 527.3496\n",
      "Epoch 13/145\n",
      "63/63 - 1s - loss: 519.7241 - mae: 519.7241 - val_loss: 648.7713 - val_mae: 648.7713\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 524.5142 - mae: 524.5142 - val_loss: 484.1170 - val_mae: 484.1170\n",
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 500.0913 - mae: 500.0913 - val_loss: 540.4795 - val_mae: 540.4795\n",
      "Epoch 16/145\n",
      "63/63 - 1s - loss: 480.6921 - mae: 480.6921 - val_loss: 482.5462 - val_mae: 482.5462\n",
      "Epoch 17/145\n",
      "63/63 - 1s - loss: 470.3400 - mae: 470.3400 - val_loss: 477.3369 - val_mae: 477.3369\n",
      "Epoch 18/145\n",
      "63/63 - 1s - loss: 506.3154 - mae: 506.3154 - val_loss: 525.0094 - val_mae: 525.0094\n",
      "Epoch 19/145\n",
      "63/63 - 1s - loss: 490.5114 - mae: 490.5114 - val_loss: 468.8177 - val_mae: 468.8177\n",
      "Epoch 20/145\n",
      "63/63 - 1s - loss: 460.0736 - mae: 460.0736 - val_loss: 471.7964 - val_mae: 471.7964\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 1s - loss: 440.4420 - mae: 440.4420 - val_loss: 498.3871 - val_mae: 498.3871\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 437.6884 - mae: 437.6884 - val_loss: 461.1418 - val_mae: 461.1418\n",
      "Epoch 23/145\n",
      "63/63 - 1s - loss: 439.8998 - mae: 439.8998 - val_loss: 457.3213 - val_mae: 457.3213\n",
      "Epoch 24/145\n",
      "63/63 - 1s - loss: 433.2890 - mae: 433.2890 - val_loss: 480.1240 - val_mae: 480.1240\n",
      "Epoch 25/145\n",
      "63/63 - 1s - loss: 435.7001 - mae: 435.7001 - val_loss: 454.1847 - val_mae: 454.1847\n",
      "Epoch 26/145\n",
      "63/63 - 1s - loss: 431.6779 - mae: 431.6779 - val_loss: 454.7134 - val_mae: 454.7134\n",
      "Epoch 27/145\n",
      "63/63 - 1s - loss: 432.2836 - mae: 432.2836 - val_loss: 459.0562 - val_mae: 459.0562\n",
      "Epoch 28/145\n",
      "63/63 - 1s - loss: 458.7172 - mae: 458.7172 - val_loss: 537.8356 - val_mae: 537.8356\n",
      "Epoch 29/145\n",
      "63/63 - 1s - loss: 439.5794 - mae: 439.5794 - val_loss: 449.5699 - val_mae: 449.5699\n",
      "Epoch 30/145\n",
      "63/63 - 1s - loss: 426.0117 - mae: 426.0117 - val_loss: 446.4696 - val_mae: 446.4696\n",
      "Epoch 31/145\n",
      "63/63 - 1s - loss: 448.3279 - mae: 448.3279 - val_loss: 460.0437 - val_mae: 460.0437\n",
      "Epoch 32/145\n",
      "63/63 - 1s - loss: 428.2573 - mae: 428.2573 - val_loss: 446.2729 - val_mae: 446.2729\n",
      "Epoch 33/145\n",
      "63/63 - 1s - loss: 427.0996 - mae: 427.0996 - val_loss: 442.4766 - val_mae: 442.4766\n",
      "Epoch 34/145\n",
      "63/63 - 1s - loss: 428.5228 - mae: 428.5228 - val_loss: 448.1989 - val_mae: 448.1989\n",
      "Epoch 35/145\n",
      "63/63 - 1s - loss: 425.7005 - mae: 425.7005 - val_loss: 453.1261 - val_mae: 453.1261\n",
      "Epoch 36/145\n",
      "63/63 - 1s - loss: 434.2977 - mae: 434.2977 - val_loss: 455.5593 - val_mae: 455.5593\n",
      "Epoch 37/145\n",
      "63/63 - 1s - loss: 420.6033 - mae: 420.6033 - val_loss: 442.9212 - val_mae: 442.9212\n",
      "Epoch 38/145\n",
      "63/63 - 1s - loss: 421.3878 - mae: 421.3878 - val_loss: 455.7000 - val_mae: 455.7000\n",
      "Epoch 39/145\n",
      "63/63 - 1s - loss: 420.8434 - mae: 420.8434 - val_loss: 462.9757 - val_mae: 462.9757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 442.1925 - mae: 442.1925 - val_loss: 456.8714 - val_mae: 456.8714\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 406.6389 - mae: 406.6389 - val_loss: 446.1329 - val_mae: 446.1329\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 403.1006 - mae: 403.1006 - val_loss: 439.0726 - val_mae: 439.0726\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 400.3065 - mae: 400.3065 - val_loss: 438.6960 - val_mae: 438.6960\n",
      "Epoch 44/145\n",
      "63/63 - 1s - loss: 400.9860 - mae: 400.9860 - val_loss: 442.9516 - val_mae: 442.9516\n",
      "Epoch 45/145\n",
      "63/63 - 1s - loss: 416.7345 - mae: 416.7345 - val_loss: 437.0114 - val_mae: 437.0114\n",
      "Epoch 46/145\n",
      "63/63 - 1s - loss: 399.0310 - mae: 399.0310 - val_loss: 436.9010 - val_mae: 436.9010\n",
      "Epoch 47/145\n",
      "63/63 - 1s - loss: 399.5738 - mae: 399.5738 - val_loss: 440.8683 - val_mae: 440.8683\n",
      "Epoch 48/145\n",
      "63/63 - 1s - loss: 399.4825 - mae: 399.4825 - val_loss: 443.8055 - val_mae: 443.8055\n",
      "Epoch 49/145\n",
      "63/63 - 1s - loss: 408.4734 - mae: 408.4734 - val_loss: 440.3185 - val_mae: 440.3185\n",
      "Epoch 50/145\n",
      "63/63 - 1s - loss: 399.2774 - mae: 399.2774 - val_loss: 436.2025 - val_mae: 436.2025\n",
      "Epoch 51/145\n",
      "63/63 - 1s - loss: 397.8420 - mae: 397.8420 - val_loss: 441.5101 - val_mae: 441.5101\n",
      "Epoch 52/145\n",
      "63/63 - 1s - loss: 413.4284 - mae: 413.4284 - val_loss: 493.2344 - val_mae: 493.2344\n",
      "Epoch 53/145\n",
      "63/63 - 1s - loss: 406.9423 - mae: 406.9423 - val_loss: 442.1234 - val_mae: 442.1234\n",
      "Epoch 54/145\n",
      "63/63 - 1s - loss: 395.6405 - mae: 395.6405 - val_loss: 447.1639 - val_mae: 447.1639\n",
      "Epoch 55/145\n",
      "63/63 - 1s - loss: 397.2197 - mae: 397.2197 - val_loss: 436.7184 - val_mae: 436.7184\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 398.0748 - mae: 398.0748 - val_loss: 442.7298 - val_mae: 442.7298\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 397.0224 - mae: 397.0224 - val_loss: 440.2990 - val_mae: 440.2990\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 393.9173 - mae: 393.9173 - val_loss: 435.3457 - val_mae: 435.3457\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 394.4827 - mae: 394.4827 - val_loss: 446.4262 - val_mae: 446.4262\n",
      "Epoch 60/145\n",
      "63/63 - 1s - loss: 404.8868 - mae: 404.8868 - val_loss: 445.7886 - val_mae: 445.7886\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 1s - loss: 387.2622 - mae: 387.2622 - val_loss: 431.7165 - val_mae: 431.7165\n",
      "Epoch 62/145\n",
      "63/63 - 1s - loss: 383.7232 - mae: 383.7232 - val_loss: 433.3401 - val_mae: 433.3401\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 383.8948 - mae: 383.8948 - val_loss: 435.9916 - val_mae: 435.9916\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 386.1567 - mae: 386.1567 - val_loss: 432.6966 - val_mae: 432.6966\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 382.8127 - mae: 382.8127 - val_loss: 436.5802 - val_mae: 436.5802\n",
      "Epoch 66/145\n",
      "63/63 - 1s - loss: 384.4562 - mae: 384.4562 - val_loss: 435.6236 - val_mae: 435.6236\n",
      "Epoch 67/145\n",
      "63/63 - 1s - loss: 384.0067 - mae: 384.0067 - val_loss: 431.9549 - val_mae: 431.9549\n",
      "Epoch 68/145\n",
      "63/63 - 1s - loss: 383.2600 - mae: 383.2600 - val_loss: 432.9479 - val_mae: 432.9479\n",
      "Epoch 69/145\n",
      "63/63 - 1s - loss: 383.2702 - mae: 383.2702 - val_loss: 433.1009 - val_mae: 433.1009\n",
      "Epoch 70/145\n",
      "63/63 - 1s - loss: 383.6787 - mae: 383.6787 - val_loss: 435.6914 - val_mae: 435.6914\n",
      "Epoch 71/145\n",
      "63/63 - 1s - loss: 381.0303 - mae: 381.0303 - val_loss: 430.1284 - val_mae: 430.1284\n",
      "Epoch 72/145\n",
      "63/63 - 1s - loss: 378.7599 - mae: 378.7599 - val_loss: 432.5543 - val_mae: 432.5543\n",
      "Epoch 73/145\n",
      "63/63 - 1s - loss: 380.4004 - mae: 380.4004 - val_loss: 433.0052 - val_mae: 433.0052\n",
      "Epoch 74/145\n",
      "63/63 - 1s - loss: 380.6737 - mae: 380.6737 - val_loss: 430.7268 - val_mae: 430.7268\n",
      "Epoch 75/145\n",
      "63/63 - 1s - loss: 380.9372 - mae: 380.9372 - val_loss: 431.6685 - val_mae: 431.6685\n",
      "Epoch 76/145\n",
      "63/63 - 1s - loss: 376.8983 - mae: 376.8983 - val_loss: 431.5052 - val_mae: 431.5052\n",
      "Epoch 77/145\n",
      "63/63 - 1s - loss: 379.2340 - mae: 379.2340 - val_loss: 436.2370 - val_mae: 436.2370\n",
      "Epoch 78/145\n",
      "63/63 - 1s - loss: 381.4370 - mae: 381.4370 - val_loss: 433.8645 - val_mae: 433.8645\n",
      "Epoch 79/145\n",
      "63/63 - 1s - loss: 375.3115 - mae: 375.3115 - val_loss: 432.3575 - val_mae: 432.3575\n",
      "Epoch 80/145\n",
      "63/63 - 1s - loss: 380.9322 - mae: 380.9322 - val_loss: 429.8069 - val_mae: 429.8069\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 1s - loss: 372.1200 - mae: 372.1200 - val_loss: 428.4030 - val_mae: 428.4030\n",
      "Epoch 82/145\n",
      "63/63 - 1s - loss: 371.0483 - mae: 371.0483 - val_loss: 429.0735 - val_mae: 429.0735\n",
      "Epoch 83/145\n",
      "63/63 - 1s - loss: 371.0488 - mae: 371.0488 - val_loss: 432.6827 - val_mae: 432.6827\n",
      "Epoch 84/145\n",
      "63/63 - 1s - loss: 371.3163 - mae: 371.3163 - val_loss: 428.4020 - val_mae: 428.4020\n",
      "Epoch 85/145\n",
      "63/63 - 1s - loss: 369.6231 - mae: 369.6231 - val_loss: 428.7651 - val_mae: 428.7651\n",
      "Epoch 86/145\n",
      "63/63 - 1s - loss: 369.8040 - mae: 369.8040 - val_loss: 427.1627 - val_mae: 427.1627\n",
      "Epoch 87/145\n",
      "63/63 - 1s - loss: 369.9312 - mae: 369.9312 - val_loss: 428.7317 - val_mae: 428.7317\n",
      "Epoch 88/145\n",
      "63/63 - 1s - loss: 369.8997 - mae: 369.8997 - val_loss: 428.6611 - val_mae: 428.6611\n",
      "Epoch 89/145\n",
      "63/63 - 1s - loss: 369.5608 - mae: 369.5608 - val_loss: 428.1776 - val_mae: 428.1776\n",
      "Epoch 90/145\n",
      "63/63 - 1s - loss: 367.8179 - mae: 367.8179 - val_loss: 427.9944 - val_mae: 427.9944\n",
      "Epoch 91/145\n",
      "63/63 - 1s - loss: 368.0395 - mae: 368.0395 - val_loss: 429.4525 - val_mae: 429.4525\n",
      "Epoch 92/145\n",
      "63/63 - 1s - loss: 367.4144 - mae: 367.4144 - val_loss: 433.6807 - val_mae: 433.6807\n",
      "Epoch 93/145\n",
      "63/63 - 1s - loss: 368.5365 - mae: 368.5365 - val_loss: 433.2638 - val_mae: 433.2638\n",
      "Epoch 94/145\n",
      "63/63 - 1s - loss: 369.7744 - mae: 369.7744 - val_loss: 427.8517 - val_mae: 427.8517\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 366.8224 - mae: 366.8224 - val_loss: 430.3014 - val_mae: 430.3014\n",
      "Epoch 96/145\n",
      "63/63 - 1s - loss: 368.7712 - mae: 368.7712 - val_loss: 430.4251 - val_mae: 430.4251\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 367.4719 - mae: 367.4719 - val_loss: 428.8015 - val_mae: 428.8015\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 365.9677 - mae: 365.9677 - val_loss: 429.0506 - val_mae: 429.0506\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 366.6371 - mae: 366.6371 - val_loss: 431.4712 - val_mae: 431.4712\n",
      "Epoch 100/145\n",
      "63/63 - 1s - loss: 366.6307 - mae: 366.6307 - val_loss: 429.8762 - val_mae: 429.8762\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 1s - loss: 363.9782 - mae: 363.9782 - val_loss: 429.4827 - val_mae: 429.4827\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 362.7193 - mae: 362.7193 - val_loss: 427.4522 - val_mae: 427.4522\n",
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 362.2796 - mae: 362.2796 - val_loss: 430.7794 - val_mae: 430.7794\n",
      "Epoch 104/145\n",
      "63/63 - 1s - loss: 361.4446 - mae: 361.4446 - val_loss: 427.7837 - val_mae: 427.7837\n",
      "Epoch 105/145\n",
      "63/63 - 1s - loss: 361.5135 - mae: 361.5135 - val_loss: 428.4200 - val_mae: 428.4200\n",
      "Epoch 106/145\n",
      "63/63 - 1s - loss: 360.9341 - mae: 360.9341 - val_loss: 427.9719 - val_mae: 427.9719\n",
      "Epoch 107/145\n",
      "63/63 - 1s - loss: 361.0219 - mae: 361.0219 - val_loss: 428.4720 - val_mae: 428.4720\n",
      "Epoch 108/145\n",
      "63/63 - 1s - loss: 361.0446 - mae: 361.0446 - val_loss: 427.6743 - val_mae: 427.6743\n",
      "Epoch 109/145\n",
      "63/63 - 1s - loss: 361.3543 - mae: 361.3543 - val_loss: 428.0885 - val_mae: 428.0885\n",
      "Epoch 110/145\n",
      "63/63 - 1s - loss: 360.7818 - mae: 360.7818 - val_loss: 427.5515 - val_mae: 427.5515\n",
      "Epoch 111/145\n",
      "63/63 - 1s - loss: 360.2729 - mae: 360.2729 - val_loss: 428.6201 - val_mae: 428.6201\n",
      "Epoch 112/145\n",
      "63/63 - 1s - loss: 360.5822 - mae: 360.5822 - val_loss: 428.3686 - val_mae: 428.3686\n",
      "Epoch 113/145\n",
      "63/63 - 1s - loss: 359.7647 - mae: 359.7647 - val_loss: 428.4049 - val_mae: 428.4049\n",
      "Epoch 114/145\n",
      "63/63 - 1s - loss: 360.5229 - mae: 360.5229 - val_loss: 428.7839 - val_mae: 428.7839\n",
      "Epoch 115/145\n",
      "63/63 - 1s - loss: 359.9681 - mae: 359.9681 - val_loss: 429.9239 - val_mae: 429.9239\n",
      "Epoch 116/145\n",
      "63/63 - 1s - loss: 358.9142 - mae: 358.9142 - val_loss: 427.9763 - val_mae: 427.9763\n",
      "Epoch 117/145\n",
      "63/63 - 1s - loss: 359.5451 - mae: 359.5451 - val_loss: 427.6828 - val_mae: 427.6828\n",
      "Epoch 118/145\n",
      "63/63 - 1s - loss: 358.7663 - mae: 358.7663 - val_loss: 428.3275 - val_mae: 428.3275\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 358.9928 - mae: 358.9928 - val_loss: 428.1792 - val_mae: 428.1792\n",
      "Epoch 120/145\n",
      "63/63 - 1s - loss: 358.5843 - mae: 358.5843 - val_loss: 428.1465 - val_mae: 428.1465\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 356.5142 - mae: 356.5142 - val_loss: 428.5918 - val_mae: 428.5918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/145\n",
      "63/63 - 1s - loss: 356.5604 - mae: 356.5604 - val_loss: 427.7741 - val_mae: 427.7741\n",
      "Epoch 123/145\n",
      "63/63 - 1s - loss: 356.4935 - mae: 356.4935 - val_loss: 428.4892 - val_mae: 428.4892\n",
      "Epoch 124/145\n",
      "63/63 - 1s - loss: 356.4485 - mae: 356.4485 - val_loss: 428.7938 - val_mae: 428.7938\n",
      "Epoch 125/145\n",
      "63/63 - 1s - loss: 356.3582 - mae: 356.3582 - val_loss: 428.1001 - val_mae: 428.1001\n",
      "Epoch 126/145\n",
      "63/63 - 1s - loss: 356.3052 - mae: 356.3052 - val_loss: 428.5671 - val_mae: 428.5671\n",
      "Epoch 127/145\n",
      "63/63 - 1s - loss: 355.4116 - mae: 355.4116 - val_loss: 428.3115 - val_mae: 428.3115\n",
      "Epoch 128/145\n",
      "63/63 - 1s - loss: 355.6132 - mae: 355.6132 - val_loss: 427.6711 - val_mae: 427.6711\n",
      "Epoch 129/145\n",
      "63/63 - 1s - loss: 355.2769 - mae: 355.2769 - val_loss: 427.9124 - val_mae: 427.9124\n",
      "Epoch 130/145\n",
      "63/63 - 1s - loss: 355.5967 - mae: 355.5967 - val_loss: 428.7119 - val_mae: 428.7119\n",
      "Epoch 131/145\n",
      "63/63 - 1s - loss: 354.8536 - mae: 354.8536 - val_loss: 428.6156 - val_mae: 428.6156\n",
      "Epoch 132/145\n",
      "63/63 - 1s - loss: 355.2390 - mae: 355.2390 - val_loss: 429.2056 - val_mae: 429.2056\n",
      "Epoch 133/145\n",
      "63/63 - 1s - loss: 354.9433 - mae: 354.9433 - val_loss: 428.8583 - val_mae: 428.8583\n",
      "Epoch 134/145\n",
      "63/63 - 1s - loss: 354.8485 - mae: 354.8485 - val_loss: 428.2587 - val_mae: 428.2587\n",
      "Epoch 135/145\n",
      "63/63 - 1s - loss: 355.1629 - mae: 355.1629 - val_loss: 428.4352 - val_mae: 428.4352\n",
      "Epoch 136/145\n",
      "63/63 - 1s - loss: 354.2030 - mae: 354.2030 - val_loss: 429.7443 - val_mae: 429.7443\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 354.8185 - mae: 354.8185 - val_loss: 429.3768 - val_mae: 429.3768\n",
      "Epoch 138/145\n",
      "63/63 - 1s - loss: 354.6165 - mae: 354.6165 - val_loss: 430.8302 - val_mae: 430.8302\n",
      "Epoch 139/145\n",
      "63/63 - 1s - loss: 354.5658 - mae: 354.5658 - val_loss: 429.1042 - val_mae: 429.1042\n",
      "Epoch 140/145\n",
      "63/63 - 1s - loss: 354.1456 - mae: 354.1456 - val_loss: 429.0647 - val_mae: 429.0647\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 352.9469 - mae: 352.9469 - val_loss: 428.4907 - val_mae: 428.4907\n",
      "Epoch 142/145\n",
      "63/63 - 1s - loss: 352.5449 - mae: 352.5449 - val_loss: 429.9947 - val_mae: 429.9947\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 353.0617 - mae: 353.0617 - val_loss: 428.5144 - val_mae: 428.5144\n",
      "Epoch 144/145\n",
      "63/63 - 1s - loss: 352.4987 - mae: 352.4987 - val_loss: 428.7427 - val_mae: 428.7427\n",
      "Epoch 145/145\n",
      "63/63 - 1s - loss: 352.0816 - mae: 352.0816 - val_loss: 428.1994 - val_mae: 428.1994\n",
      "\n",
      "val_mae is:428.1994274354005\n",
      "\n",
      "fold: 3\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2787.2751 - mae: 2787.2751 - val_loss: 1016.2506 - val_mae: 1016.2507\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 836.2949 - mae: 836.2949 - val_loss: 718.6025 - val_mae: 718.6025\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 675.1887 - mae: 675.1887 - val_loss: 619.2957 - val_mae: 619.2957\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 659.1775 - mae: 659.1775 - val_loss: 656.8532 - val_mae: 656.8532\n",
      "Epoch 5/145\n",
      "63/63 - 1s - loss: 632.1656 - mae: 632.1656 - val_loss: 583.0060 - val_mae: 583.0060\n",
      "Epoch 6/145\n",
      "63/63 - 1s - loss: 568.2377 - mae: 568.2377 - val_loss: 537.0382 - val_mae: 537.0382\n",
      "Epoch 7/145\n",
      "63/63 - 1s - loss: 544.1084 - mae: 544.1084 - val_loss: 527.7516 - val_mae: 527.7516\n",
      "Epoch 8/145\n",
      "63/63 - 1s - loss: 538.6121 - mae: 538.6121 - val_loss: 518.7529 - val_mae: 518.7529\n",
      "Epoch 9/145\n",
      "63/63 - 1s - loss: 527.8186 - mae: 527.8186 - val_loss: 552.7357 - val_mae: 552.7357\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 527.3336 - mae: 527.3336 - val_loss: 561.0937 - val_mae: 561.0937\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 520.2587 - mae: 520.2587 - val_loss: 505.1482 - val_mae: 505.1482\n",
      "Epoch 12/145\n",
      "63/63 - 1s - loss: 500.4933 - mae: 500.4933 - val_loss: 497.0684 - val_mae: 497.0684\n",
      "Epoch 13/145\n",
      "63/63 - 1s - loss: 496.1402 - mae: 496.1402 - val_loss: 615.3140 - val_mae: 615.3140\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 564.5991 - mae: 564.5991 - val_loss: 493.2107 - val_mae: 493.2107\n",
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 487.3040 - mae: 487.3040 - val_loss: 481.4740 - val_mae: 481.4740\n",
      "Epoch 16/145\n",
      "63/63 - 1s - loss: 545.7204 - mae: 545.7204 - val_loss: 532.6494 - val_mae: 532.6494\n",
      "Epoch 17/145\n",
      "63/63 - 1s - loss: 492.1123 - mae: 492.1123 - val_loss: 474.0467 - val_mae: 474.0467\n",
      "Epoch 18/145\n",
      "63/63 - 1s - loss: 468.4968 - mae: 468.4968 - val_loss: 481.5517 - val_mae: 481.5517\n",
      "Epoch 19/145\n",
      "63/63 - 1s - loss: 469.4864 - mae: 469.4864 - val_loss: 476.0369 - val_mae: 476.0369\n",
      "Epoch 20/145\n",
      "63/63 - 1s - loss: 486.5670 - mae: 486.5670 - val_loss: 486.7128 - val_mae: 486.7128\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 1s - loss: 447.9586 - mae: 447.9586 - val_loss: 453.9642 - val_mae: 453.9642\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 443.3736 - mae: 443.3736 - val_loss: 457.8411 - val_mae: 457.8411\n",
      "Epoch 23/145\n",
      "63/63 - 1s - loss: 441.7510 - mae: 441.7510 - val_loss: 462.4920 - val_mae: 462.4920\n",
      "Epoch 24/145\n",
      "63/63 - 1s - loss: 443.4826 - mae: 443.4826 - val_loss: 451.6137 - val_mae: 451.6137\n",
      "Epoch 25/145\n",
      "63/63 - 1s - loss: 437.4711 - mae: 437.4711 - val_loss: 448.5936 - val_mae: 448.5936\n",
      "Epoch 26/145\n",
      "63/63 - 1s - loss: 438.7096 - mae: 438.7096 - val_loss: 450.6206 - val_mae: 450.6206\n",
      "Epoch 27/145\n",
      "63/63 - 1s - loss: 439.9126 - mae: 439.9126 - val_loss: 458.9197 - val_mae: 458.9197\n",
      "Epoch 28/145\n",
      "63/63 - 1s - loss: 437.9805 - mae: 437.9805 - val_loss: 449.1075 - val_mae: 449.1075\n",
      "Epoch 29/145\n",
      "63/63 - 1s - loss: 438.1181 - mae: 438.1181 - val_loss: 444.1808 - val_mae: 444.1808\n",
      "Epoch 30/145\n",
      "63/63 - 1s - loss: 432.8176 - mae: 432.8176 - val_loss: 446.9116 - val_mae: 446.9116\n",
      "Epoch 31/145\n",
      "63/63 - 1s - loss: 430.6782 - mae: 430.6782 - val_loss: 452.7953 - val_mae: 452.7953\n",
      "Epoch 32/145\n",
      "63/63 - 1s - loss: 432.5854 - mae: 432.5854 - val_loss: 478.0216 - val_mae: 478.0216\n",
      "Epoch 33/145\n",
      "63/63 - 1s - loss: 443.5350 - mae: 443.5350 - val_loss: 444.8966 - val_mae: 444.8966\n",
      "Epoch 34/145\n",
      "63/63 - 1s - loss: 426.8379 - mae: 426.8379 - val_loss: 445.9564 - val_mae: 445.9564\n",
      "Epoch 35/145\n",
      "63/63 - 1s - loss: 450.4535 - mae: 450.4535 - val_loss: 451.5027 - val_mae: 451.5027\n",
      "Epoch 36/145\n",
      "63/63 - 1s - loss: 430.0407 - mae: 430.0407 - val_loss: 444.5323 - val_mae: 444.5323\n",
      "Epoch 37/145\n",
      "63/63 - 1s - loss: 431.0804 - mae: 431.0804 - val_loss: 444.2887 - val_mae: 444.2887\n",
      "Epoch 38/145\n",
      "63/63 - 1s - loss: 427.1952 - mae: 427.1952 - val_loss: 439.3929 - val_mae: 439.3929\n",
      "Epoch 39/145\n",
      "63/63 - 1s - loss: 425.4361 - mae: 425.4361 - val_loss: 463.5742 - val_mae: 463.5742\n",
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 427.4470 - mae: 427.4470 - val_loss: 442.6642 - val_mae: 442.6642\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 413.1031 - mae: 413.1031 - val_loss: 435.8842 - val_mae: 435.8842\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 409.8473 - mae: 409.8473 - val_loss: 440.6487 - val_mae: 440.6487\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 409.3604 - mae: 409.3604 - val_loss: 432.4844 - val_mae: 432.4844\n",
      "Epoch 44/145\n",
      "63/63 - 1s - loss: 407.7153 - mae: 407.7153 - val_loss: 433.2728 - val_mae: 433.2728\n",
      "Epoch 45/145\n",
      "63/63 - 1s - loss: 407.2656 - mae: 407.2656 - val_loss: 438.6470 - val_mae: 438.6470\n",
      "Epoch 46/145\n",
      "63/63 - 1s - loss: 410.0381 - mae: 410.0381 - val_loss: 432.4444 - val_mae: 432.4444\n",
      "Epoch 47/145\n",
      "63/63 - 1s - loss: 405.9796 - mae: 405.9796 - val_loss: 431.0780 - val_mae: 431.0780\n",
      "Epoch 48/145\n",
      "63/63 - 1s - loss: 409.7549 - mae: 409.7549 - val_loss: 443.7774 - val_mae: 443.7774\n",
      "Epoch 49/145\n",
      "63/63 - 1s - loss: 408.1457 - mae: 408.1457 - val_loss: 431.4978 - val_mae: 431.4978\n",
      "Epoch 50/145\n",
      "63/63 - 1s - loss: 402.3542 - mae: 402.3542 - val_loss: 429.0342 - val_mae: 429.0342\n",
      "Epoch 51/145\n",
      "63/63 - 1s - loss: 407.0326 - mae: 407.0326 - val_loss: 430.4771 - val_mae: 430.4771\n",
      "Epoch 52/145\n",
      "63/63 - 1s - loss: 402.4491 - mae: 402.4491 - val_loss: 430.7231 - val_mae: 430.7231\n",
      "Epoch 53/145\n",
      "63/63 - 1s - loss: 402.6081 - mae: 402.6081 - val_loss: 446.9289 - val_mae: 446.9289\n",
      "Epoch 54/145\n",
      "63/63 - 1s - loss: 401.2294 - mae: 401.2294 - val_loss: 430.7087 - val_mae: 430.7087\n",
      "Epoch 55/145\n",
      "63/63 - 1s - loss: 402.9881 - mae: 402.9881 - val_loss: 427.6259 - val_mae: 427.6259\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 401.5699 - mae: 401.5699 - val_loss: 430.2798 - val_mae: 430.2798\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 400.6292 - mae: 400.6292 - val_loss: 441.3369 - val_mae: 441.3369\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 413.0914 - mae: 413.0914 - val_loss: 446.0582 - val_mae: 446.0582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 402.7613 - mae: 402.7613 - val_loss: 426.9366 - val_mae: 426.9366\n",
      "Epoch 60/145\n",
      "63/63 - 1s - loss: 401.6692 - mae: 401.6692 - val_loss: 426.2747 - val_mae: 426.2747\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 1s - loss: 390.3248 - mae: 390.3248 - val_loss: 422.5131 - val_mae: 422.5131\n",
      "Epoch 62/145\n",
      "63/63 - 1s - loss: 391.9995 - mae: 391.9995 - val_loss: 422.8121 - val_mae: 422.8121\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 390.2666 - mae: 390.2666 - val_loss: 425.0165 - val_mae: 425.0165\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 389.3222 - mae: 389.3222 - val_loss: 424.4386 - val_mae: 424.4386\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 392.3930 - mae: 392.3930 - val_loss: 429.6477 - val_mae: 429.6477\n",
      "Epoch 66/145\n",
      "63/63 - 1s - loss: 388.9505 - mae: 388.9505 - val_loss: 423.9532 - val_mae: 423.9532\n",
      "Epoch 67/145\n",
      "63/63 - 1s - loss: 387.6384 - mae: 387.6384 - val_loss: 421.8427 - val_mae: 421.8427\n",
      "Epoch 68/145\n",
      "63/63 - 1s - loss: 389.5551 - mae: 389.5551 - val_loss: 425.5286 - val_mae: 425.5286\n",
      "Epoch 69/145\n",
      "63/63 - 1s - loss: 386.9764 - mae: 386.9764 - val_loss: 423.9987 - val_mae: 423.9987\n",
      "Epoch 70/145\n",
      "63/63 - 1s - loss: 387.4411 - mae: 387.4411 - val_loss: 422.0273 - val_mae: 422.0273\n",
      "Epoch 71/145\n",
      "63/63 - 1s - loss: 386.0380 - mae: 386.0380 - val_loss: 425.6028 - val_mae: 425.6028\n",
      "Epoch 72/145\n",
      "63/63 - 1s - loss: 387.9049 - mae: 387.9049 - val_loss: 425.6400 - val_mae: 425.6400\n",
      "Epoch 73/145\n",
      "63/63 - 1s - loss: 389.7207 - mae: 389.7207 - val_loss: 422.6877 - val_mae: 422.6877\n",
      "Epoch 74/145\n",
      "63/63 - 1s - loss: 390.3331 - mae: 390.3331 - val_loss: 423.7303 - val_mae: 423.7303\n",
      "Epoch 75/145\n",
      "63/63 - 1s - loss: 384.8228 - mae: 384.8228 - val_loss: 422.9864 - val_mae: 422.9864\n",
      "Epoch 76/145\n",
      "63/63 - 1s - loss: 387.1640 - mae: 387.1640 - val_loss: 427.8173 - val_mae: 427.8173\n",
      "Epoch 77/145\n",
      "63/63 - 1s - loss: 383.8885 - mae: 383.8885 - val_loss: 421.2218 - val_mae: 421.2218\n",
      "Epoch 78/145\n",
      "63/63 - 1s - loss: 386.4577 - mae: 386.4577 - val_loss: 424.3043 - val_mae: 424.3043\n",
      "Epoch 79/145\n",
      "63/63 - 1s - loss: 382.9301 - mae: 382.9301 - val_loss: 422.1446 - val_mae: 422.1446\n",
      "Epoch 80/145\n",
      "63/63 - 1s - loss: 384.2293 - mae: 384.2293 - val_loss: 425.8593 - val_mae: 425.8593\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 1s - loss: 379.6018 - mae: 379.6018 - val_loss: 420.5155 - val_mae: 420.5155\n",
      "Epoch 82/145\n",
      "63/63 - 1s - loss: 377.6400 - mae: 377.6400 - val_loss: 419.7345 - val_mae: 419.7345\n",
      "Epoch 83/145\n",
      "63/63 - 1s - loss: 377.8748 - mae: 377.8748 - val_loss: 422.5091 - val_mae: 422.5091\n",
      "Epoch 84/145\n",
      "63/63 - 1s - loss: 378.3457 - mae: 378.3457 - val_loss: 419.1388 - val_mae: 419.1388\n",
      "Epoch 85/145\n",
      "63/63 - 1s - loss: 377.2195 - mae: 377.2195 - val_loss: 418.3721 - val_mae: 418.3721\n",
      "Epoch 86/145\n",
      "63/63 - 1s - loss: 376.9749 - mae: 376.9749 - val_loss: 419.3486 - val_mae: 419.3486\n",
      "Epoch 87/145\n",
      "63/63 - 1s - loss: 377.1441 - mae: 377.1441 - val_loss: 420.3329 - val_mae: 420.3329\n",
      "Epoch 88/145\n",
      "63/63 - 1s - loss: 376.8267 - mae: 376.8267 - val_loss: 419.8116 - val_mae: 419.8116\n",
      "Epoch 89/145\n",
      "63/63 - 1s - loss: 375.6382 - mae: 375.6382 - val_loss: 419.8410 - val_mae: 419.8410\n",
      "Epoch 90/145\n",
      "63/63 - 1s - loss: 376.6813 - mae: 376.6813 - val_loss: 422.2982 - val_mae: 422.2982\n",
      "Epoch 91/145\n",
      "63/63 - 1s - loss: 378.3363 - mae: 378.3363 - val_loss: 419.8122 - val_mae: 419.8122\n",
      "Epoch 92/145\n",
      "63/63 - 1s - loss: 375.6229 - mae: 375.6229 - val_loss: 421.8737 - val_mae: 421.8737\n",
      "Epoch 93/145\n",
      "63/63 - 1s - loss: 375.4435 - mae: 375.4435 - val_loss: 420.1644 - val_mae: 420.1644\n",
      "Epoch 94/145\n",
      "63/63 - 1s - loss: 375.3018 - mae: 375.3018 - val_loss: 420.3696 - val_mae: 420.3696\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 375.6805 - mae: 375.6805 - val_loss: 419.6346 - val_mae: 419.6346\n",
      "Epoch 96/145\n",
      "63/63 - 1s - loss: 374.1403 - mae: 374.1403 - val_loss: 423.1014 - val_mae: 423.1014\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 375.6760 - mae: 375.6760 - val_loss: 421.3467 - val_mae: 421.3467\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 376.0157 - mae: 376.0157 - val_loss: 420.2298 - val_mae: 420.2298\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 374.6167 - mae: 374.6167 - val_loss: 419.2580 - val_mae: 419.2580\n",
      "Epoch 100/145\n",
      "63/63 - 1s - loss: 372.7277 - mae: 372.7277 - val_loss: 418.3811 - val_mae: 418.3811\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 1s - loss: 371.3401 - mae: 371.3401 - val_loss: 418.7090 - val_mae: 418.7090\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 371.2206 - mae: 371.2206 - val_loss: 419.4795 - val_mae: 419.4795\n",
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 370.2631 - mae: 370.2631 - val_loss: 419.1888 - val_mae: 419.1888\n",
      "Epoch 104/145\n",
      "63/63 - 1s - loss: 370.8568 - mae: 370.8568 - val_loss: 417.9163 - val_mae: 417.9163\n",
      "Epoch 105/145\n",
      "63/63 - 1s - loss: 370.3210 - mae: 370.3210 - val_loss: 418.9207 - val_mae: 418.9207\n",
      "Epoch 106/145\n",
      "63/63 - 1s - loss: 370.1712 - mae: 370.1712 - val_loss: 419.3722 - val_mae: 419.3722\n",
      "Epoch 107/145\n",
      "63/63 - 1s - loss: 370.1742 - mae: 370.1742 - val_loss: 417.5928 - val_mae: 417.5928\n",
      "Epoch 108/145\n",
      "63/63 - 1s - loss: 369.7124 - mae: 369.7124 - val_loss: 419.1045 - val_mae: 419.1045\n",
      "Epoch 109/145\n",
      "63/63 - 1s - loss: 369.4535 - mae: 369.4535 - val_loss: 418.8241 - val_mae: 418.8241\n",
      "Epoch 110/145\n",
      "63/63 - 1s - loss: 369.9719 - mae: 369.9719 - val_loss: 419.3510 - val_mae: 419.3510\n",
      "Epoch 111/145\n",
      "63/63 - 1s - loss: 369.0155 - mae: 369.0155 - val_loss: 418.9584 - val_mae: 418.9584\n",
      "Epoch 112/145\n",
      "63/63 - 1s - loss: 369.4450 - mae: 369.4450 - val_loss: 418.1834 - val_mae: 418.1834\n",
      "Epoch 113/145\n",
      "63/63 - 1s - loss: 368.7539 - mae: 368.7539 - val_loss: 417.3510 - val_mae: 417.3510\n",
      "Epoch 114/145\n",
      "63/63 - 1s - loss: 368.7307 - mae: 368.7307 - val_loss: 418.2778 - val_mae: 418.2778\n",
      "Epoch 115/145\n",
      "63/63 - 1s - loss: 368.7635 - mae: 368.7635 - val_loss: 417.8152 - val_mae: 417.8152\n",
      "Epoch 116/145\n",
      "63/63 - 1s - loss: 368.4860 - mae: 368.4860 - val_loss: 417.9578 - val_mae: 417.9578\n",
      "Epoch 117/145\n",
      "63/63 - 1s - loss: 368.1503 - mae: 368.1503 - val_loss: 418.8606 - val_mae: 418.8606\n",
      "Epoch 118/145\n",
      "63/63 - 1s - loss: 367.8374 - mae: 367.8374 - val_loss: 418.4645 - val_mae: 418.4645\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 367.6146 - mae: 367.6146 - val_loss: 418.2468 - val_mae: 418.2468\n",
      "Epoch 120/145\n",
      "63/63 - 1s - loss: 367.3820 - mae: 367.3820 - val_loss: 417.6976 - val_mae: 417.6976\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 366.2270 - mae: 366.2270 - val_loss: 418.0466 - val_mae: 418.0466\n",
      "Epoch 122/145\n",
      "63/63 - 1s - loss: 365.5012 - mae: 365.5012 - val_loss: 417.3280 - val_mae: 417.3280\n",
      "Epoch 123/145\n",
      "63/63 - 1s - loss: 365.6530 - mae: 365.6530 - val_loss: 417.2631 - val_mae: 417.2631\n",
      "Epoch 124/145\n",
      "63/63 - 1s - loss: 365.3222 - mae: 365.3222 - val_loss: 417.9866 - val_mae: 417.9866\n",
      "Epoch 125/145\n",
      "63/63 - 1s - loss: 365.4591 - mae: 365.4591 - val_loss: 417.9922 - val_mae: 417.9922\n",
      "Epoch 126/145\n",
      "63/63 - 1s - loss: 365.8142 - mae: 365.8142 - val_loss: 418.0180 - val_mae: 418.0180\n",
      "Epoch 127/145\n",
      "63/63 - 1s - loss: 364.9737 - mae: 364.9737 - val_loss: 417.5978 - val_mae: 417.5978\n",
      "Epoch 128/145\n",
      "63/63 - 1s - loss: 365.1381 - mae: 365.1381 - val_loss: 417.0674 - val_mae: 417.0674\n",
      "Epoch 129/145\n",
      "63/63 - 1s - loss: 364.8070 - mae: 364.8070 - val_loss: 417.3666 - val_mae: 417.3666\n",
      "Epoch 130/145\n",
      "63/63 - 1s - loss: 364.2341 - mae: 364.2341 - val_loss: 416.9470 - val_mae: 416.9470\n",
      "Epoch 131/145\n",
      "63/63 - 1s - loss: 364.2343 - mae: 364.2343 - val_loss: 418.6068 - val_mae: 418.6068\n",
      "Epoch 132/145\n",
      "63/63 - 1s - loss: 365.2222 - mae: 365.2222 - val_loss: 417.9288 - val_mae: 417.9288\n",
      "Epoch 133/145\n",
      "63/63 - 1s - loss: 364.2899 - mae: 364.2899 - val_loss: 417.9025 - val_mae: 417.9025\n",
      "Epoch 134/145\n",
      "63/63 - 1s - loss: 364.1093 - mae: 364.1093 - val_loss: 417.2621 - val_mae: 417.2621\n",
      "Epoch 135/145\n",
      "63/63 - 1s - loss: 363.9478 - mae: 363.9478 - val_loss: 417.8154 - val_mae: 417.8154\n",
      "Epoch 136/145\n",
      "63/63 - 1s - loss: 363.9709 - mae: 363.9709 - val_loss: 417.9599 - val_mae: 417.9599\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 363.4922 - mae: 363.4922 - val_loss: 417.4296 - val_mae: 417.4296\n",
      "Epoch 138/145\n",
      "63/63 - 1s - loss: 363.7963 - mae: 363.7963 - val_loss: 418.2967 - val_mae: 418.2967\n",
      "Epoch 139/145\n",
      "63/63 - 1s - loss: 364.2607 - mae: 364.2607 - val_loss: 417.1454 - val_mae: 417.1454\n",
      "Epoch 140/145\n",
      "63/63 - 1s - loss: 363.5600 - mae: 363.5600 - val_loss: 417.6846 - val_mae: 417.6846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 362.2347 - mae: 362.2347 - val_loss: 417.0142 - val_mae: 417.0142\n",
      "Epoch 142/145\n",
      "63/63 - 1s - loss: 362.2676 - mae: 362.2676 - val_loss: 417.0424 - val_mae: 417.0424\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 362.4232 - mae: 362.4232 - val_loss: 417.0135 - val_mae: 417.0135\n",
      "Epoch 144/145\n",
      "63/63 - 1s - loss: 362.2274 - mae: 362.2274 - val_loss: 418.0527 - val_mae: 418.0527\n",
      "Epoch 145/145\n",
      "63/63 - 1s - loss: 361.8947 - mae: 361.8947 - val_loss: 417.3794 - val_mae: 417.3794\n",
      "\n",
      "val_mae is:417.37943368774415\n",
      "\n",
      "fold: 4\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2515.1609 - mae: 2515.1609 - val_loss: 888.0394 - val_mae: 888.0394\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 796.9133 - mae: 796.9133 - val_loss: 681.8772 - val_mae: 681.8772\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 707.0456 - mae: 707.0456 - val_loss: 624.8246 - val_mae: 624.8246\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 613.5575 - mae: 613.5575 - val_loss: 650.8071 - val_mae: 650.8071\n",
      "Epoch 5/145\n",
      "63/63 - 1s - loss: 595.8278 - mae: 595.8278 - val_loss: 550.5125 - val_mae: 550.5125\n",
      "Epoch 6/145\n",
      "63/63 - 1s - loss: 571.0948 - mae: 571.0948 - val_loss: 567.7545 - val_mae: 567.7545\n",
      "Epoch 7/145\n",
      "63/63 - 1s - loss: 552.1810 - mae: 552.1810 - val_loss: 528.4585 - val_mae: 528.4585\n",
      "Epoch 8/145\n",
      "63/63 - 1s - loss: 539.8303 - mae: 539.8303 - val_loss: 603.7665 - val_mae: 603.7665\n",
      "Epoch 9/145\n",
      "63/63 - 1s - loss: 582.2232 - mae: 582.2232 - val_loss: 512.1151 - val_mae: 512.1151\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 512.4872 - mae: 512.4872 - val_loss: 525.1758 - val_mae: 525.1758\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 537.4930 - mae: 537.4930 - val_loss: 493.3529 - val_mae: 493.3529\n",
      "Epoch 12/145\n",
      "63/63 - 1s - loss: 498.5904 - mae: 498.5904 - val_loss: 504.2823 - val_mae: 504.2823\n",
      "Epoch 13/145\n",
      "63/63 - 1s - loss: 500.3685 - mae: 500.3685 - val_loss: 498.5383 - val_mae: 498.5383\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 552.3318 - mae: 552.3318 - val_loss: 497.7800 - val_mae: 497.7800\n",
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 548.3702 - mae: 548.3702 - val_loss: 561.2051 - val_mae: 561.2051\n",
      "Epoch 16/145\n",
      "63/63 - 1s - loss: 501.9039 - mae: 501.9039 - val_loss: 574.9825 - val_mae: 574.9825\n",
      "Epoch 17/145\n",
      "63/63 - 1s - loss: 542.2380 - mae: 542.2380 - val_loss: 475.0974 - val_mae: 475.0974\n",
      "Epoch 18/145\n",
      "63/63 - 1s - loss: 496.1774 - mae: 496.1774 - val_loss: 542.0019 - val_mae: 542.0019\n",
      "Epoch 19/145\n",
      "63/63 - 1s - loss: 527.1826 - mae: 527.1826 - val_loss: 577.8605 - val_mae: 577.8605\n",
      "Epoch 20/145\n",
      "63/63 - 1s - loss: 475.4208 - mae: 475.4208 - val_loss: 466.0430 - val_mae: 466.0430\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 1s - loss: 448.8066 - mae: 448.8066 - val_loss: 451.1443 - val_mae: 451.1443\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 447.6493 - mae: 447.6493 - val_loss: 452.5487 - val_mae: 452.5487\n",
      "Epoch 23/145\n",
      "63/63 - 1s - loss: 441.6105 - mae: 441.6105 - val_loss: 446.3000 - val_mae: 446.3000\n",
      "Epoch 24/145\n",
      "63/63 - 1s - loss: 441.6327 - mae: 441.6327 - val_loss: 450.7317 - val_mae: 450.7317\n",
      "Epoch 25/145\n",
      "63/63 - 1s - loss: 442.4305 - mae: 442.4305 - val_loss: 448.7581 - val_mae: 448.7581\n",
      "Epoch 26/145\n",
      "63/63 - 1s - loss: 444.3759 - mae: 444.3759 - val_loss: 446.6013 - val_mae: 446.6013\n",
      "Epoch 27/145\n",
      "63/63 - 1s - loss: 443.5463 - mae: 443.5463 - val_loss: 473.7775 - val_mae: 473.7775\n",
      "Epoch 28/145\n",
      "63/63 - 1s - loss: 464.0824 - mae: 464.0824 - val_loss: 518.8298 - val_mae: 518.8298\n",
      "Epoch 29/145\n",
      "63/63 - 1s - loss: 458.7438 - mae: 458.7438 - val_loss: 442.8356 - val_mae: 442.8356\n",
      "Epoch 30/145\n",
      "63/63 - 1s - loss: 438.7226 - mae: 438.7226 - val_loss: 440.7963 - val_mae: 440.7963\n",
      "Epoch 31/145\n",
      "63/63 - 1s - loss: 439.0313 - mae: 439.0313 - val_loss: 446.1052 - val_mae: 446.1052\n",
      "Epoch 32/145\n",
      "63/63 - 1s - loss: 434.9217 - mae: 434.9217 - val_loss: 470.2964 - val_mae: 470.2964\n",
      "Epoch 33/145\n",
      "63/63 - 1s - loss: 450.2218 - mae: 450.2218 - val_loss: 443.0898 - val_mae: 443.0898\n",
      "Epoch 34/145\n",
      "63/63 - 1s - loss: 440.5279 - mae: 440.5279 - val_loss: 502.1362 - val_mae: 502.1362\n",
      "Epoch 35/145\n",
      "63/63 - 1s - loss: 465.0657 - mae: 465.0657 - val_loss: 448.5577 - val_mae: 448.5577\n",
      "Epoch 36/145\n",
      "63/63 - 1s - loss: 429.7229 - mae: 429.7229 - val_loss: 464.7783 - val_mae: 464.7783\n",
      "Epoch 37/145\n",
      "63/63 - 1s - loss: 433.6081 - mae: 433.6081 - val_loss: 438.5151 - val_mae: 438.5151\n",
      "Epoch 38/145\n",
      "63/63 - 1s - loss: 430.6852 - mae: 430.6852 - val_loss: 477.3111 - val_mae: 477.3111\n",
      "Epoch 39/145\n",
      "63/63 - 1s - loss: 431.8136 - mae: 431.8136 - val_loss: 442.1720 - val_mae: 442.1720\n",
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 439.9302 - mae: 439.9302 - val_loss: 439.4931 - val_mae: 439.4931\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 417.7635 - mae: 417.7635 - val_loss: 432.1692 - val_mae: 432.1692\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 412.6208 - mae: 412.6208 - val_loss: 427.6073 - val_mae: 427.6073\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 414.1599 - mae: 414.1599 - val_loss: 430.1784 - val_mae: 430.1784\n",
      "Epoch 44/145\n",
      "63/63 - 1s - loss: 415.2074 - mae: 415.2074 - val_loss: 462.4499 - val_mae: 462.4499\n",
      "Epoch 45/145\n",
      "63/63 - 1s - loss: 422.5713 - mae: 422.5713 - val_loss: 427.5474 - val_mae: 427.5474\n",
      "Epoch 46/145\n",
      "63/63 - 1s - loss: 411.0061 - mae: 411.0061 - val_loss: 427.0687 - val_mae: 427.0687\n",
      "Epoch 47/145\n",
      "63/63 - 1s - loss: 410.1056 - mae: 410.1056 - val_loss: 431.1139 - val_mae: 431.1139\n",
      "Epoch 48/145\n",
      "63/63 - 1s - loss: 414.1573 - mae: 414.1573 - val_loss: 435.3173 - val_mae: 435.3173\n",
      "Epoch 49/145\n",
      "63/63 - 1s - loss: 419.7767 - mae: 419.7767 - val_loss: 429.4666 - val_mae: 429.4666\n",
      "Epoch 50/145\n",
      "63/63 - 1s - loss: 409.4261 - mae: 409.4261 - val_loss: 432.7508 - val_mae: 432.7508\n",
      "Epoch 51/145\n",
      "63/63 - 1s - loss: 409.2650 - mae: 409.2650 - val_loss: 432.6095 - val_mae: 432.6095\n",
      "Epoch 52/145\n",
      "63/63 - 1s - loss: 419.1640 - mae: 419.1640 - val_loss: 434.8156 - val_mae: 434.8156\n",
      "Epoch 53/145\n",
      "63/63 - 1s - loss: 415.7681 - mae: 415.7681 - val_loss: 428.7103 - val_mae: 428.7103\n",
      "Epoch 54/145\n",
      "63/63 - 1s - loss: 409.3795 - mae: 409.3795 - val_loss: 433.4110 - val_mae: 433.4110\n",
      "Epoch 55/145\n",
      "63/63 - 1s - loss: 404.4619 - mae: 404.4619 - val_loss: 432.5206 - val_mae: 432.5206\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 405.5259 - mae: 405.5259 - val_loss: 424.4742 - val_mae: 424.4742\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 405.6832 - mae: 405.6832 - val_loss: 421.1409 - val_mae: 421.1409\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 401.4630 - mae: 401.4630 - val_loss: 436.5895 - val_mae: 436.5895\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 411.8137 - mae: 411.8137 - val_loss: 423.7010 - val_mae: 423.7010\n",
      "Epoch 60/145\n",
      "63/63 - 1s - loss: 401.3243 - mae: 401.3243 - val_loss: 426.4364 - val_mae: 426.4364\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 1s - loss: 394.2942 - mae: 394.2942 - val_loss: 419.4026 - val_mae: 419.4026\n",
      "Epoch 62/145\n",
      "63/63 - 1s - loss: 393.0959 - mae: 393.0959 - val_loss: 418.0312 - val_mae: 418.0312\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 392.9590 - mae: 392.9590 - val_loss: 427.8674 - val_mae: 427.8674\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 392.7228 - mae: 392.7228 - val_loss: 418.9798 - val_mae: 418.9798\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 390.7498 - mae: 390.7498 - val_loss: 418.5825 - val_mae: 418.5825\n",
      "Epoch 66/145\n",
      "63/63 - 1s - loss: 389.8464 - mae: 389.8464 - val_loss: 420.9855 - val_mae: 420.9855\n",
      "Epoch 67/145\n",
      "63/63 - 1s - loss: 391.5664 - mae: 391.5664 - val_loss: 418.0882 - val_mae: 418.0882\n",
      "Epoch 68/145\n",
      "63/63 - 1s - loss: 390.8094 - mae: 390.8094 - val_loss: 416.9014 - val_mae: 416.9014\n",
      "Epoch 69/145\n",
      "63/63 - 1s - loss: 389.7560 - mae: 389.7560 - val_loss: 416.9793 - val_mae: 416.9793\n",
      "Epoch 70/145\n",
      "63/63 - 1s - loss: 387.0241 - mae: 387.0241 - val_loss: 420.2182 - val_mae: 420.2182\n",
      "Epoch 71/145\n",
      "63/63 - 1s - loss: 389.1800 - mae: 389.1800 - val_loss: 416.8258 - val_mae: 416.8258\n",
      "Epoch 72/145\n",
      "63/63 - 1s - loss: 386.9450 - mae: 386.9450 - val_loss: 416.4373 - val_mae: 416.4373\n",
      "Epoch 73/145\n",
      "63/63 - 1s - loss: 388.0420 - mae: 388.0420 - val_loss: 415.9198 - val_mae: 415.9198\n",
      "Epoch 74/145\n",
      "63/63 - 1s - loss: 386.7509 - mae: 386.7509 - val_loss: 421.3782 - val_mae: 421.3782\n",
      "Epoch 75/145\n",
      "63/63 - 1s - loss: 388.4431 - mae: 388.4431 - val_loss: 417.1570 - val_mae: 417.1570\n",
      "Epoch 76/145\n",
      "63/63 - 1s - loss: 386.0010 - mae: 386.0010 - val_loss: 418.3695 - val_mae: 418.3695\n",
      "Epoch 77/145\n",
      "63/63 - 1s - loss: 384.7430 - mae: 384.7430 - val_loss: 418.2581 - val_mae: 418.2581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/145\n",
      "63/63 - 1s - loss: 384.4962 - mae: 384.4962 - val_loss: 419.4508 - val_mae: 419.4508\n",
      "Epoch 79/145\n",
      "63/63 - 1s - loss: 385.3644 - mae: 385.3644 - val_loss: 416.0286 - val_mae: 416.0286\n",
      "Epoch 80/145\n",
      "63/63 - 1s - loss: 383.3365 - mae: 383.3365 - val_loss: 415.3479 - val_mae: 415.3479\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 1s - loss: 378.7584 - mae: 378.7584 - val_loss: 415.2724 - val_mae: 415.2724\n",
      "Epoch 82/145\n",
      "63/63 - 1s - loss: 378.5179 - mae: 378.5179 - val_loss: 414.5436 - val_mae: 414.5436\n",
      "Epoch 83/145\n",
      "63/63 - 1s - loss: 378.1747 - mae: 378.1747 - val_loss: 414.3937 - val_mae: 414.3937\n",
      "Epoch 84/145\n",
      "63/63 - 1s - loss: 377.1210 - mae: 377.1210 - val_loss: 413.6130 - val_mae: 413.6130\n",
      "Epoch 85/145\n",
      "63/63 - 1s - loss: 378.2303 - mae: 378.2303 - val_loss: 417.7773 - val_mae: 417.7773\n",
      "Epoch 86/145\n",
      "63/63 - 1s - loss: 376.7776 - mae: 376.7776 - val_loss: 415.0431 - val_mae: 415.0431\n",
      "Epoch 87/145\n",
      "63/63 - 1s - loss: 377.0661 - mae: 377.0661 - val_loss: 413.8941 - val_mae: 413.8941\n",
      "Epoch 88/145\n",
      "63/63 - 1s - loss: 375.7274 - mae: 375.7274 - val_loss: 412.1353 - val_mae: 412.1353\n",
      "Epoch 89/145\n",
      "63/63 - 1s - loss: 376.8307 - mae: 376.8307 - val_loss: 417.1485 - val_mae: 417.1485\n",
      "Epoch 90/145\n",
      "63/63 - 1s - loss: 376.6434 - mae: 376.6434 - val_loss: 412.3751 - val_mae: 412.3751\n",
      "Epoch 91/145\n",
      "63/63 - 1s - loss: 374.2649 - mae: 374.2649 - val_loss: 412.6988 - val_mae: 412.6988\n",
      "Epoch 92/145\n",
      "63/63 - 1s - loss: 376.2912 - mae: 376.2912 - val_loss: 413.5698 - val_mae: 413.5698\n",
      "Epoch 93/145\n",
      "63/63 - 1s - loss: 374.3828 - mae: 374.3828 - val_loss: 412.7455 - val_mae: 412.7455\n",
      "Epoch 94/145\n",
      "63/63 - 1s - loss: 375.5810 - mae: 375.5810 - val_loss: 412.5662 - val_mae: 412.5662\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 373.8948 - mae: 373.8948 - val_loss: 419.3799 - val_mae: 419.3799\n",
      "Epoch 96/145\n",
      "63/63 - 1s - loss: 375.9367 - mae: 375.9367 - val_loss: 412.7169 - val_mae: 412.7169\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 373.8242 - mae: 373.8242 - val_loss: 413.0536 - val_mae: 413.0536\n",
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 373.9826 - mae: 373.9826 - val_loss: 412.7728 - val_mae: 412.7728\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 373.3064 - mae: 373.3064 - val_loss: 413.6407 - val_mae: 413.6407\n",
      "Epoch 100/145\n",
      "63/63 - 1s - loss: 371.6438 - mae: 371.6438 - val_loss: 412.2929 - val_mae: 412.2929\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 1s - loss: 368.9421 - mae: 368.9421 - val_loss: 411.2117 - val_mae: 411.2117\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 369.2012 - mae: 369.2012 - val_loss: 412.8577 - val_mae: 412.8577\n",
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 368.0189 - mae: 368.0189 - val_loss: 412.7419 - val_mae: 412.7419\n",
      "Epoch 104/145\n",
      "63/63 - 1s - loss: 368.0729 - mae: 368.0729 - val_loss: 413.8991 - val_mae: 413.8991\n",
      "Epoch 105/145\n",
      "63/63 - 1s - loss: 368.4711 - mae: 368.4711 - val_loss: 411.9608 - val_mae: 411.9608\n",
      "Epoch 106/145\n",
      "63/63 - 1s - loss: 367.6712 - mae: 367.6712 - val_loss: 412.4620 - val_mae: 412.4620\n",
      "Epoch 107/145\n",
      "63/63 - 1s - loss: 368.3009 - mae: 368.3009 - val_loss: 413.5728 - val_mae: 413.5728\n",
      "Epoch 108/145\n",
      "63/63 - 1s - loss: 366.7318 - mae: 366.7318 - val_loss: 411.9447 - val_mae: 411.9447\n",
      "Epoch 109/145\n",
      "63/63 - 1s - loss: 366.5991 - mae: 366.5991 - val_loss: 412.2771 - val_mae: 412.2771\n",
      "Epoch 110/145\n",
      "63/63 - 1s - loss: 367.3785 - mae: 367.3785 - val_loss: 413.5210 - val_mae: 413.5210\n",
      "Epoch 111/145\n",
      "63/63 - 1s - loss: 367.0067 - mae: 367.0067 - val_loss: 412.0864 - val_mae: 412.0864\n",
      "Epoch 112/145\n",
      "63/63 - 1s - loss: 366.5011 - mae: 366.5011 - val_loss: 412.3884 - val_mae: 412.3884\n",
      "Epoch 113/145\n",
      "63/63 - 1s - loss: 366.1256 - mae: 366.1256 - val_loss: 410.9828 - val_mae: 410.9828\n",
      "Epoch 114/145\n",
      "63/63 - 1s - loss: 366.0635 - mae: 366.0635 - val_loss: 411.8935 - val_mae: 411.8935\n",
      "Epoch 115/145\n",
      "63/63 - 1s - loss: 365.2477 - mae: 365.2477 - val_loss: 412.9281 - val_mae: 412.9281\n",
      "Epoch 116/145\n",
      "63/63 - 1s - loss: 365.3165 - mae: 365.3165 - val_loss: 413.0202 - val_mae: 413.0202\n",
      "Epoch 117/145\n",
      "63/63 - 1s - loss: 365.2747 - mae: 365.2747 - val_loss: 413.3608 - val_mae: 413.3608\n",
      "Epoch 118/145\n",
      "63/63 - 1s - loss: 364.8393 - mae: 364.8393 - val_loss: 412.8929 - val_mae: 412.8929\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 364.8623 - mae: 364.8623 - val_loss: 412.3925 - val_mae: 412.3925\n",
      "Epoch 120/145\n",
      "63/63 - 1s - loss: 364.5131 - mae: 364.5131 - val_loss: 413.8022 - val_mae: 413.8022\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 362.7770 - mae: 362.7770 - val_loss: 411.7038 - val_mae: 411.7038\n",
      "Epoch 122/145\n",
      "63/63 - 1s - loss: 362.3615 - mae: 362.3615 - val_loss: 412.0208 - val_mae: 412.0208\n",
      "Epoch 123/145\n",
      "63/63 - 1s - loss: 361.9764 - mae: 361.9764 - val_loss: 411.3038 - val_mae: 411.3038\n",
      "Epoch 124/145\n",
      "63/63 - 1s - loss: 362.1685 - mae: 362.1685 - val_loss: 411.8239 - val_mae: 411.8239\n",
      "Epoch 125/145\n",
      "63/63 - 1s - loss: 361.8812 - mae: 361.8812 - val_loss: 412.6464 - val_mae: 412.6464\n",
      "Epoch 126/145\n",
      "63/63 - 1s - loss: 362.0648 - mae: 362.0648 - val_loss: 411.8652 - val_mae: 411.8652\n",
      "Epoch 127/145\n",
      "63/63 - 1s - loss: 361.8225 - mae: 361.8225 - val_loss: 412.1188 - val_mae: 412.1188\n",
      "Epoch 128/145\n",
      "63/63 - 1s - loss: 361.2227 - mae: 361.2227 - val_loss: 411.8676 - val_mae: 411.8676\n",
      "Epoch 129/145\n",
      "63/63 - 1s - loss: 361.8630 - mae: 361.8630 - val_loss: 412.0004 - val_mae: 412.0004\n",
      "Epoch 130/145\n",
      "63/63 - 1s - loss: 361.2216 - mae: 361.2216 - val_loss: 411.7394 - val_mae: 411.7394\n",
      "Epoch 131/145\n",
      "63/63 - 1s - loss: 360.4489 - mae: 360.4489 - val_loss: 412.4428 - val_mae: 412.4428\n",
      "Epoch 132/145\n",
      "63/63 - 1s - loss: 360.4390 - mae: 360.4390 - val_loss: 412.8163 - val_mae: 412.8163\n",
      "Epoch 133/145\n",
      "63/63 - 1s - loss: 360.9468 - mae: 360.9468 - val_loss: 411.6638 - val_mae: 411.6638\n",
      "Epoch 134/145\n",
      "63/63 - 1s - loss: 360.5497 - mae: 360.5497 - val_loss: 414.1848 - val_mae: 414.1848\n",
      "Epoch 135/145\n",
      "63/63 - 1s - loss: 360.2627 - mae: 360.2627 - val_loss: 411.4049 - val_mae: 411.4049\n",
      "Epoch 136/145\n",
      "63/63 - 1s - loss: 360.4677 - mae: 360.4677 - val_loss: 413.3085 - val_mae: 413.3085\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 359.4728 - mae: 359.4728 - val_loss: 412.5250 - val_mae: 412.5250\n",
      "Epoch 138/145\n",
      "63/63 - 1s - loss: 359.9106 - mae: 359.9106 - val_loss: 413.4018 - val_mae: 413.4018\n",
      "Epoch 139/145\n",
      "63/63 - 1s - loss: 360.6305 - mae: 360.6305 - val_loss: 411.6702 - val_mae: 411.6702\n",
      "Epoch 140/145\n",
      "63/63 - 1s - loss: 360.0806 - mae: 360.0806 - val_loss: 413.3342 - val_mae: 413.3342\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 358.2727 - mae: 358.2727 - val_loss: 411.5097 - val_mae: 411.5097\n",
      "Epoch 142/145\n",
      "63/63 - 1s - loss: 357.8825 - mae: 357.8825 - val_loss: 411.4706 - val_mae: 411.4706\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 358.2809 - mae: 358.2809 - val_loss: 411.4794 - val_mae: 411.4794\n",
      "Epoch 144/145\n",
      "63/63 - 1s - loss: 357.8615 - mae: 357.8615 - val_loss: 411.5084 - val_mae: 411.5084\n",
      "Epoch 145/145\n",
      "63/63 - 1s - loss: 357.8329 - mae: 357.8329 - val_loss: 411.4989 - val_mae: 411.4989\n",
      "\n",
      "val_mae is:411.49887699386596\n",
      "\n",
      "fold: 5\n",
      "Epoch 1/145\n",
      "63/63 - 1s - loss: 2569.9502 - mae: 2569.9502 - val_loss: 864.2134 - val_mae: 864.2134\n",
      "Epoch 2/145\n",
      "63/63 - 1s - loss: 795.4277 - mae: 795.4277 - val_loss: 758.5466 - val_mae: 758.5466\n",
      "Epoch 3/145\n",
      "63/63 - 1s - loss: 674.8519 - mae: 674.8519 - val_loss: 597.1415 - val_mae: 597.1415\n",
      "Epoch 4/145\n",
      "63/63 - 1s - loss: 619.3574 - mae: 619.3574 - val_loss: 620.7582 - val_mae: 620.7582\n",
      "Epoch 5/145\n",
      "63/63 - 1s - loss: 583.4567 - mae: 583.4567 - val_loss: 531.7064 - val_mae: 531.7064\n",
      "Epoch 6/145\n",
      "63/63 - 1s - loss: 596.4955 - mae: 596.4955 - val_loss: 517.1696 - val_mae: 517.1696\n",
      "Epoch 7/145\n",
      "63/63 - 1s - loss: 541.3225 - mae: 541.3225 - val_loss: 526.4963 - val_mae: 526.4963\n",
      "Epoch 8/145\n",
      "63/63 - 1s - loss: 535.2424 - mae: 535.2424 - val_loss: 505.9365 - val_mae: 505.9365\n",
      "Epoch 9/145\n",
      "63/63 - 1s - loss: 554.9886 - mae: 554.9886 - val_loss: 661.4057 - val_mae: 661.4057\n",
      "Epoch 10/145\n",
      "63/63 - 1s - loss: 515.0031 - mae: 515.0031 - val_loss: 481.9337 - val_mae: 481.9337\n",
      "Epoch 11/145\n",
      "63/63 - 1s - loss: 509.5629 - mae: 509.5629 - val_loss: 515.4621 - val_mae: 515.4621\n",
      "Epoch 12/145\n",
      "63/63 - 1s - loss: 507.0129 - mae: 507.0129 - val_loss: 485.9380 - val_mae: 485.9380\n",
      "Epoch 13/145\n",
      "63/63 - 1s - loss: 498.8678 - mae: 498.8678 - val_loss: 484.0751 - val_mae: 484.0751\n",
      "Epoch 14/145\n",
      "63/63 - 1s - loss: 504.1355 - mae: 504.1355 - val_loss: 507.2542 - val_mae: 507.2542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/145\n",
      "63/63 - 1s - loss: 501.1956 - mae: 501.1956 - val_loss: 466.8242 - val_mae: 466.8242\n",
      "Epoch 16/145\n",
      "63/63 - 1s - loss: 507.7339 - mae: 507.7339 - val_loss: 481.3199 - val_mae: 481.3199\n",
      "Epoch 17/145\n",
      "63/63 - 1s - loss: 505.5224 - mae: 505.5224 - val_loss: 648.6382 - val_mae: 648.6382\n",
      "Epoch 18/145\n",
      "63/63 - 1s - loss: 536.9136 - mae: 536.9136 - val_loss: 457.7835 - val_mae: 457.7835\n",
      "Epoch 19/145\n",
      "63/63 - 1s - loss: 480.5285 - mae: 480.5284 - val_loss: 472.1486 - val_mae: 472.1486\n",
      "Epoch 20/145\n",
      "63/63 - 1s - loss: 511.1381 - mae: 511.1381 - val_loss: 459.7254 - val_mae: 459.7254\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      "63/63 - 1s - loss: 448.8853 - mae: 448.8853 - val_loss: 441.4634 - val_mae: 441.4634\n",
      "Epoch 22/145\n",
      "63/63 - 1s - loss: 448.6555 - mae: 448.6555 - val_loss: 436.9794 - val_mae: 436.9794\n",
      "Epoch 23/145\n",
      "63/63 - 1s - loss: 445.1579 - mae: 445.1579 - val_loss: 442.0049 - val_mae: 442.0049\n",
      "Epoch 24/145\n",
      "63/63 - 1s - loss: 465.0173 - mae: 465.0173 - val_loss: 432.8260 - val_mae: 432.8260\n",
      "Epoch 25/145\n",
      "63/63 - 1s - loss: 443.8728 - mae: 443.8728 - val_loss: 443.3384 - val_mae: 443.3384\n",
      "Epoch 26/145\n",
      "63/63 - 1s - loss: 439.9396 - mae: 439.9396 - val_loss: 448.5032 - val_mae: 448.5032\n",
      "Epoch 27/145\n",
      "63/63 - 1s - loss: 440.0656 - mae: 440.0656 - val_loss: 438.2420 - val_mae: 438.2420\n",
      "Epoch 28/145\n",
      "63/63 - 1s - loss: 439.9901 - mae: 439.9901 - val_loss: 441.2442 - val_mae: 441.2442\n",
      "Epoch 29/145\n",
      "63/63 - 1s - loss: 439.3223 - mae: 439.3223 - val_loss: 451.2462 - val_mae: 451.2462\n",
      "Epoch 30/145\n",
      "63/63 - 1s - loss: 467.6840 - mae: 467.6840 - val_loss: 514.8182 - val_mae: 514.8182\n",
      "Epoch 31/145\n",
      "63/63 - 1s - loss: 470.9521 - mae: 470.9521 - val_loss: 441.1878 - val_mae: 441.1878\n",
      "Epoch 32/145\n",
      "63/63 - 1s - loss: 435.0432 - mae: 435.0432 - val_loss: 438.6007 - val_mae: 438.6007\n",
      "Epoch 33/145\n",
      "63/63 - 1s - loss: 437.3338 - mae: 437.3338 - val_loss: 453.1622 - val_mae: 453.1622\n",
      "Epoch 34/145\n",
      "63/63 - 1s - loss: 433.3548 - mae: 433.3548 - val_loss: 431.3167 - val_mae: 431.3167\n",
      "Epoch 35/145\n",
      "63/63 - 1s - loss: 432.0533 - mae: 432.0533 - val_loss: 440.3889 - val_mae: 440.3889\n",
      "Epoch 36/145\n",
      "63/63 - 1s - loss: 429.4540 - mae: 429.4540 - val_loss: 433.1267 - val_mae: 433.1267\n",
      "Epoch 37/145\n",
      "63/63 - 1s - loss: 432.3302 - mae: 432.3302 - val_loss: 437.7986 - val_mae: 437.7986\n",
      "Epoch 38/145\n",
      "63/63 - 1s - loss: 430.9472 - mae: 430.9472 - val_loss: 432.3804 - val_mae: 432.3804\n",
      "Epoch 39/145\n",
      "63/63 - 1s - loss: 431.0927 - mae: 431.0927 - val_loss: 452.9129 - val_mae: 452.9129\n",
      "Epoch 40/145\n",
      "63/63 - 1s - loss: 428.0089 - mae: 428.0089 - val_loss: 437.8652 - val_mae: 437.8652\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      "63/63 - 1s - loss: 415.3428 - mae: 415.3428 - val_loss: 429.3896 - val_mae: 429.3896\n",
      "Epoch 42/145\n",
      "63/63 - 1s - loss: 429.5322 - mae: 429.5322 - val_loss: 448.2043 - val_mae: 448.2043\n",
      "Epoch 43/145\n",
      "63/63 - 1s - loss: 428.9374 - mae: 428.9374 - val_loss: 422.5954 - val_mae: 422.5954\n",
      "Epoch 44/145\n",
      "63/63 - 1s - loss: 411.1753 - mae: 411.1753 - val_loss: 416.5156 - val_mae: 416.5156\n",
      "Epoch 45/145\n",
      "63/63 - 1s - loss: 408.5765 - mae: 408.5765 - val_loss: 421.6270 - val_mae: 421.6270\n",
      "Epoch 46/145\n",
      "63/63 - 1s - loss: 411.5717 - mae: 411.5717 - val_loss: 419.1670 - val_mae: 419.1670\n",
      "Epoch 47/145\n",
      "63/63 - 1s - loss: 410.0982 - mae: 410.0982 - val_loss: 421.9465 - val_mae: 421.9465\n",
      "Epoch 48/145\n",
      "63/63 - 1s - loss: 409.6656 - mae: 409.6656 - val_loss: 433.7352 - val_mae: 433.7352\n",
      "Epoch 49/145\n",
      "63/63 - 1s - loss: 408.7081 - mae: 408.7081 - val_loss: 418.2868 - val_mae: 418.2868\n",
      "Epoch 50/145\n",
      "63/63 - 1s - loss: 407.5546 - mae: 407.5546 - val_loss: 419.3138 - val_mae: 419.3138\n",
      "Epoch 51/145\n",
      "63/63 - 1s - loss: 408.9979 - mae: 408.9979 - val_loss: 415.3123 - val_mae: 415.3123\n",
      "Epoch 52/145\n",
      "63/63 - 1s - loss: 412.7193 - mae: 412.7193 - val_loss: 470.7684 - val_mae: 470.7684\n",
      "Epoch 53/145\n",
      "63/63 - 1s - loss: 416.7193 - mae: 416.7193 - val_loss: 417.5970 - val_mae: 417.5970\n",
      "Epoch 54/145\n",
      "63/63 - 1s - loss: 405.9856 - mae: 405.9856 - val_loss: 416.8538 - val_mae: 416.8538\n",
      "Epoch 55/145\n",
      "63/63 - 1s - loss: 404.2687 - mae: 404.2687 - val_loss: 417.4717 - val_mae: 417.4717\n",
      "Epoch 56/145\n",
      "63/63 - 1s - loss: 403.1753 - mae: 403.1753 - val_loss: 424.2034 - val_mae: 424.2034\n",
      "Epoch 57/145\n",
      "63/63 - 1s - loss: 406.5750 - mae: 406.5750 - val_loss: 415.1608 - val_mae: 415.1608\n",
      "Epoch 58/145\n",
      "63/63 - 1s - loss: 402.1159 - mae: 402.1159 - val_loss: 423.3703 - val_mae: 423.3703\n",
      "Epoch 59/145\n",
      "63/63 - 1s - loss: 405.5655 - mae: 405.5655 - val_loss: 434.2350 - val_mae: 434.2350\n",
      "Epoch 60/145\n",
      "63/63 - 1s - loss: 404.3167 - mae: 404.3167 - val_loss: 417.4782 - val_mae: 417.4782\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      "63/63 - 1s - loss: 394.9494 - mae: 394.9494 - val_loss: 418.7974 - val_mae: 418.7974\n",
      "Epoch 62/145\n",
      "63/63 - 1s - loss: 393.9212 - mae: 393.9212 - val_loss: 411.2777 - val_mae: 411.2777\n",
      "Epoch 63/145\n",
      "63/63 - 1s - loss: 392.6731 - mae: 392.6731 - val_loss: 414.0328 - val_mae: 414.0328\n",
      "Epoch 64/145\n",
      "63/63 - 1s - loss: 390.7884 - mae: 390.7884 - val_loss: 412.8518 - val_mae: 412.8518\n",
      "Epoch 65/145\n",
      "63/63 - 1s - loss: 390.5914 - mae: 390.5914 - val_loss: 412.0562 - val_mae: 412.0562\n",
      "Epoch 66/145\n",
      "63/63 - 1s - loss: 390.2018 - mae: 390.2018 - val_loss: 411.6234 - val_mae: 411.6234\n",
      "Epoch 67/145\n",
      "63/63 - 1s - loss: 389.1403 - mae: 389.1403 - val_loss: 412.9003 - val_mae: 412.9003\n",
      "Epoch 68/145\n",
      "63/63 - 1s - loss: 388.6173 - mae: 388.6173 - val_loss: 409.9919 - val_mae: 409.9919\n",
      "Epoch 69/145\n",
      "63/63 - 1s - loss: 389.2633 - mae: 389.2633 - val_loss: 417.3241 - val_mae: 417.3241\n",
      "Epoch 70/145\n",
      "63/63 - 1s - loss: 389.0444 - mae: 389.0444 - val_loss: 414.8189 - val_mae: 414.8189\n",
      "Epoch 71/145\n",
      "63/63 - 1s - loss: 389.1170 - mae: 389.1170 - val_loss: 417.4046 - val_mae: 417.4046\n",
      "Epoch 72/145\n",
      "63/63 - 1s - loss: 388.1774 - mae: 388.1774 - val_loss: 408.8187 - val_mae: 408.8187\n",
      "Epoch 73/145\n",
      "63/63 - 1s - loss: 386.3080 - mae: 386.3080 - val_loss: 410.0800 - val_mae: 410.0800\n",
      "Epoch 74/145\n",
      "63/63 - 1s - loss: 390.1997 - mae: 390.1997 - val_loss: 420.4699 - val_mae: 420.4699\n",
      "Epoch 75/145\n",
      "63/63 - 1s - loss: 389.2321 - mae: 389.2321 - val_loss: 414.5182 - val_mae: 414.5182\n",
      "Epoch 76/145\n",
      "63/63 - 1s - loss: 385.3216 - mae: 385.3216 - val_loss: 410.1990 - val_mae: 410.1990\n",
      "Epoch 77/145\n",
      "63/63 - 1s - loss: 384.0207 - mae: 384.0207 - val_loss: 413.0793 - val_mae: 413.0793\n",
      "Epoch 78/145\n",
      "63/63 - 1s - loss: 384.1780 - mae: 384.1780 - val_loss: 413.0938 - val_mae: 413.0938\n",
      "Epoch 79/145\n",
      "63/63 - 1s - loss: 383.3931 - mae: 383.3931 - val_loss: 413.3678 - val_mae: 413.3678\n",
      "Epoch 80/145\n",
      "63/63 - 1s - loss: 382.9030 - mae: 382.9030 - val_loss: 410.1048 - val_mae: 410.1048\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      "63/63 - 1s - loss: 378.4255 - mae: 378.4255 - val_loss: 408.5351 - val_mae: 408.5351\n",
      "Epoch 82/145\n",
      "63/63 - 1s - loss: 377.9493 - mae: 377.9493 - val_loss: 408.1297 - val_mae: 408.1297\n",
      "Epoch 83/145\n",
      "63/63 - 1s - loss: 379.6788 - mae: 379.6788 - val_loss: 408.6834 - val_mae: 408.6834\n",
      "Epoch 84/145\n",
      "63/63 - 1s - loss: 378.1114 - mae: 378.1114 - val_loss: 406.4373 - val_mae: 406.4373\n",
      "Epoch 85/145\n",
      "63/63 - 1s - loss: 377.9452 - mae: 377.9452 - val_loss: 408.8006 - val_mae: 408.8006\n",
      "Epoch 86/145\n",
      "63/63 - 1s - loss: 376.8260 - mae: 376.8260 - val_loss: 408.7329 - val_mae: 408.7329\n",
      "Epoch 87/145\n",
      "63/63 - 1s - loss: 376.0337 - mae: 376.0337 - val_loss: 409.8839 - val_mae: 409.8839\n",
      "Epoch 88/145\n",
      "63/63 - 1s - loss: 376.3400 - mae: 376.3400 - val_loss: 408.2150 - val_mae: 408.2150\n",
      "Epoch 89/145\n",
      "63/63 - 1s - loss: 375.6743 - mae: 375.6743 - val_loss: 408.6369 - val_mae: 408.6369\n",
      "Epoch 90/145\n",
      "63/63 - 1s - loss: 378.4908 - mae: 378.4908 - val_loss: 413.8000 - val_mae: 413.8000\n",
      "Epoch 91/145\n",
      "63/63 - 1s - loss: 376.3542 - mae: 376.3542 - val_loss: 408.7258 - val_mae: 408.7258\n",
      "Epoch 92/145\n",
      "63/63 - 1s - loss: 375.0882 - mae: 375.0882 - val_loss: 407.8024 - val_mae: 407.8024\n",
      "Epoch 93/145\n",
      "63/63 - 1s - loss: 374.6143 - mae: 374.6143 - val_loss: 411.7873 - val_mae: 411.7873\n",
      "Epoch 94/145\n",
      "63/63 - 1s - loss: 374.2940 - mae: 374.2940 - val_loss: 408.7106 - val_mae: 408.7106\n",
      "Epoch 95/145\n",
      "63/63 - 1s - loss: 374.8597 - mae: 374.8597 - val_loss: 409.8056 - val_mae: 409.8056\n",
      "Epoch 96/145\n",
      "63/63 - 1s - loss: 372.1670 - mae: 372.1670 - val_loss: 407.6497 - val_mae: 407.6497\n",
      "Epoch 97/145\n",
      "63/63 - 1s - loss: 373.2225 - mae: 373.2225 - val_loss: 408.9293 - val_mae: 408.9293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/145\n",
      "63/63 - 1s - loss: 372.5159 - mae: 372.5159 - val_loss: 408.2253 - val_mae: 408.2253\n",
      "Epoch 99/145\n",
      "63/63 - 1s - loss: 373.3732 - mae: 373.3732 - val_loss: 409.6261 - val_mae: 409.6261\n",
      "Epoch 100/145\n",
      "63/63 - 1s - loss: 373.4946 - mae: 373.4946 - val_loss: 409.1254 - val_mae: 409.1254\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      "63/63 - 1s - loss: 369.5492 - mae: 369.5492 - val_loss: 407.1517 - val_mae: 407.1517\n",
      "Epoch 102/145\n",
      "63/63 - 1s - loss: 369.1087 - mae: 369.1087 - val_loss: 408.9312 - val_mae: 408.9312\n",
      "Epoch 103/145\n",
      "63/63 - 1s - loss: 368.3390 - mae: 368.3390 - val_loss: 406.6789 - val_mae: 406.6789\n",
      "Epoch 104/145\n",
      "63/63 - 1s - loss: 368.4200 - mae: 368.4200 - val_loss: 406.4166 - val_mae: 406.4166\n",
      "Epoch 105/145\n",
      "63/63 - 1s - loss: 367.7512 - mae: 367.7512 - val_loss: 406.4414 - val_mae: 406.4414\n",
      "Epoch 106/145\n",
      "63/63 - 1s - loss: 367.8327 - mae: 367.8327 - val_loss: 406.5229 - val_mae: 406.5229\n",
      "Epoch 107/145\n",
      "63/63 - 1s - loss: 368.7365 - mae: 368.7365 - val_loss: 413.1500 - val_mae: 413.1500\n",
      "Epoch 108/145\n",
      "63/63 - 1s - loss: 368.9033 - mae: 368.9033 - val_loss: 407.6313 - val_mae: 407.6313\n",
      "Epoch 109/145\n",
      "63/63 - 1s - loss: 367.4869 - mae: 367.4869 - val_loss: 406.1238 - val_mae: 406.1238\n",
      "Epoch 110/145\n",
      "63/63 - 1s - loss: 367.5102 - mae: 367.5102 - val_loss: 406.3257 - val_mae: 406.3257\n",
      "Epoch 111/145\n",
      "63/63 - 1s - loss: 366.8368 - mae: 366.8368 - val_loss: 407.7806 - val_mae: 407.7806\n",
      "Epoch 112/145\n",
      "63/63 - 1s - loss: 366.4344 - mae: 366.4344 - val_loss: 405.6724 - val_mae: 405.6724\n",
      "Epoch 113/145\n",
      "63/63 - 1s - loss: 365.9744 - mae: 365.9744 - val_loss: 410.8426 - val_mae: 410.8426\n",
      "Epoch 114/145\n",
      "63/63 - 1s - loss: 366.8320 - mae: 366.8320 - val_loss: 406.9251 - val_mae: 406.9251\n",
      "Epoch 115/145\n",
      "63/63 - 1s - loss: 365.8568 - mae: 365.8568 - val_loss: 406.3115 - val_mae: 406.3115\n",
      "Epoch 116/145\n",
      "63/63 - 1s - loss: 365.7543 - mae: 365.7543 - val_loss: 406.7420 - val_mae: 406.7420\n",
      "Epoch 117/145\n",
      "63/63 - 1s - loss: 365.2194 - mae: 365.2194 - val_loss: 407.1220 - val_mae: 407.1220\n",
      "Epoch 118/145\n",
      "63/63 - 1s - loss: 365.3476 - mae: 365.3476 - val_loss: 406.7645 - val_mae: 406.7645\n",
      "Epoch 119/145\n",
      "63/63 - 1s - loss: 364.2702 - mae: 364.2702 - val_loss: 407.3400 - val_mae: 407.3400\n",
      "Epoch 120/145\n",
      "63/63 - 1s - loss: 365.1165 - mae: 365.1165 - val_loss: 407.1992 - val_mae: 407.1992\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      "63/63 - 1s - loss: 362.7560 - mae: 362.7560 - val_loss: 406.6035 - val_mae: 406.6035\n",
      "Epoch 122/145\n",
      "63/63 - 1s - loss: 362.7501 - mae: 362.7501 - val_loss: 406.2953 - val_mae: 406.2953\n",
      "Epoch 123/145\n",
      "63/63 - 1s - loss: 362.3130 - mae: 362.3130 - val_loss: 405.3262 - val_mae: 405.3262\n",
      "Epoch 124/145\n",
      "63/63 - 1s - loss: 362.3089 - mae: 362.3089 - val_loss: 406.1903 - val_mae: 406.1903\n",
      "Epoch 125/145\n",
      "63/63 - 1s - loss: 362.4912 - mae: 362.4912 - val_loss: 405.6155 - val_mae: 405.6155\n",
      "Epoch 126/145\n",
      "63/63 - 1s - loss: 362.0601 - mae: 362.0601 - val_loss: 406.6935 - val_mae: 406.6935\n",
      "Epoch 127/145\n",
      "63/63 - 1s - loss: 361.9649 - mae: 361.9649 - val_loss: 406.6656 - val_mae: 406.6656\n",
      "Epoch 128/145\n",
      "63/63 - 1s - loss: 361.5522 - mae: 361.5522 - val_loss: 406.1615 - val_mae: 406.1615\n",
      "Epoch 129/145\n",
      "63/63 - 1s - loss: 361.6562 - mae: 361.6562 - val_loss: 406.3979 - val_mae: 406.3979\n",
      "Epoch 130/145\n",
      "63/63 - 1s - loss: 361.3496 - mae: 361.3496 - val_loss: 407.0035 - val_mae: 407.0035\n",
      "Epoch 131/145\n",
      "63/63 - 1s - loss: 362.1257 - mae: 362.1257 - val_loss: 407.7634 - val_mae: 407.7634\n",
      "Epoch 132/145\n",
      "63/63 - 1s - loss: 361.9690 - mae: 361.9690 - val_loss: 408.6373 - val_mae: 408.6373\n",
      "Epoch 133/145\n",
      "63/63 - 1s - loss: 361.4056 - mae: 361.4056 - val_loss: 407.1143 - val_mae: 407.1143\n",
      "Epoch 134/145\n",
      "63/63 - 1s - loss: 360.6385 - mae: 360.6385 - val_loss: 409.0755 - val_mae: 409.0755\n",
      "Epoch 135/145\n",
      "63/63 - 1s - loss: 360.9917 - mae: 360.9917 - val_loss: 405.8618 - val_mae: 405.8618\n",
      "Epoch 136/145\n",
      "63/63 - 1s - loss: 360.5628 - mae: 360.5628 - val_loss: 407.1636 - val_mae: 407.1636\n",
      "Epoch 137/145\n",
      "63/63 - 1s - loss: 360.5696 - mae: 360.5696 - val_loss: 407.0934 - val_mae: 407.0934\n",
      "Epoch 138/145\n",
      "63/63 - 1s - loss: 360.4772 - mae: 360.4772 - val_loss: 406.3725 - val_mae: 406.3725\n",
      "Epoch 139/145\n",
      "63/63 - 1s - loss: 360.1572 - mae: 360.1572 - val_loss: 405.9152 - val_mae: 405.9152\n",
      "Epoch 140/145\n",
      "63/63 - 1s - loss: 359.9507 - mae: 359.9507 - val_loss: 406.6045 - val_mae: 406.6045\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      "63/63 - 1s - loss: 358.6559 - mae: 358.6559 - val_loss: 406.0462 - val_mae: 406.0462\n",
      "Epoch 142/145\n",
      "63/63 - 1s - loss: 358.2750 - mae: 358.2750 - val_loss: 406.1007 - val_mae: 406.1007\n",
      "Epoch 143/145\n",
      "63/63 - 1s - loss: 358.5872 - mae: 358.5872 - val_loss: 405.9879 - val_mae: 405.9879\n",
      "Epoch 144/145\n",
      "63/63 - 1s - loss: 358.1682 - mae: 358.1682 - val_loss: 407.0067 - val_mae: 407.0067\n",
      "Epoch 145/145\n",
      "63/63 - 1s - loss: 358.0351 - mae: 358.0351 - val_loss: 406.1069 - val_mae: 406.1069\n",
      "\n",
      "val_mae is:406.10686093753816\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "416.2007160488721"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits = 6\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "import keras \n",
    "\n",
    "b_size = 2000\n",
    "max_epochs = 145\n",
    "oof_pred = np.zeros((len(X_pca), ))\n",
    "\n",
    "sub = pd.read_csv('./used_car_testB_20200421.csv',sep = ' ')[['SaleID']].copy()\n",
    "sub['price'] = 0\n",
    "\n",
    "avg_mae = 0\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X_pca, y)):\n",
    "    print('fold:', fold)\n",
    "    X_train, y_train = X_pca[trn_idx], y[trn_idx]\n",
    "    X_val, y_val = X_pca[val_idx], y[val_idx]\n",
    "    \n",
    "    model = NN_model(X_train.shape[1])\n",
    "    simple_adam = keras.optimizers.Adam(lr = 0.015)\n",
    "    \n",
    "    model.compile(loss='mae', optimizer=simple_adam,metrics=['mae'])\n",
    "    es = EarlyStopping(monitor='val_score', patience=10, verbose=2, mode='min', restore_best_weights=True,)\n",
    "    es.set_model(model)\n",
    "    metric = Metric(model, [es], [(X_train, y_train), (X_val, y_val)])\n",
    "    model.fit(X_train, y_train, batch_size=b_size, epochs=max_epochs, \n",
    "              validation_data = (X_val, y_val),\n",
    "              callbacks=[reduce_lr], shuffle=True, verbose=2)\n",
    "    y_pred3 = model.predict(X_val)\n",
    "    y_pred = np.zeros((len(y_pred3), ))\n",
    "    sub['price'] += model.predict(test).reshape(-1,)/n_splits\n",
    "    for i in range(len(y_pred3)):\n",
    "        y_pred[i] = y_pred3[i]\n",
    "        \n",
    "    oof_pred[val_idx] = y_pred\n",
    "    val_mae = mean_absolute_error(y[val_idx], y_pred)\n",
    "    avg_mae += val_mae/n_splits\n",
    "    print()\n",
    "    print('val_mae is:{}'.format(val_mae))\n",
    "    print()\n",
    "mean_absolute_error(y, oof_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:24:18.711716Z",
     "start_time": "2021-04-16T16:24:18.572585Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.to_csv('nn_sub_{}_{}.csv'.format('mae', sub['price'].mean()), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
